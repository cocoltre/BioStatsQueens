---
title: '**Predicting Mortgage Yield using Regression Analysis**'
author: 'Group 42: Clara Delandre, Majandra Garcia, Paola Biocchi, Coline Leteurtre'
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
    toc: false
    latex_engine: pdflatex
    keep_tex: true
  html_document:
    toc: false
    df_print: paged
bibliography: refs.bib
geometry: top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm
fontsize: 12pt
header-includes:
  - \usepackage{graphicx}
  - \usepackage{amsmath}
  - \usepackage{booktabs}
  - \usepackage[font=footnotesize,skip=4pt]{caption}
  - \usepackage{caption}
  - \usepackage{fancyhdr}
  - \usepackage{ragged2e}
  - \usepackage{multicol}
  - \justifying
  - \pagestyle{fancy}
  - \fancyhead[L]{Delandre, Garcia, Biocchi, Leteurtre}
  - \fancyfoot[C]{\thepage}
  - \usepackage{titlesec}
  - \titlespacing*{\title}{0pt}{0pt}{0pt}
  - "\\titlespacing*{\\author}{0pt}{-0.5cm}{0pt}"
  - "\\titlespacing*{\\date}{0pt}{-0.5cm}{0pt}"
  - \titlespacing*{\section}{0pt}{0.5cm}{0pt}
  - \titlespacing*{\subsection}{0pt}{0.2cm}{0pt}
  - \titlespacing*{\subsubsection}{0pt}{0.2cm}{0pt}
  - \usepackage{etoolbox}
  - \patchcmd{\maketitle}{\vspace*{2\baselineskip}}{}{}{}
  - \usepackage{titling}
  - "\\setlength{\\droptitle}{-1.5cm}"
  - \setlength{\headheight}{12pt}
  - \setlength{\headsep}{5pt}
  - \setlength{\parindent}{0pt}
  - \setlength{\parskip}{0.5pt}
  - |
    \pretitle{\begin{center}\large}
    \posttitle{\end{center}}
    \preauthor{\begin{center}\small}
    \postauthor{\end{center}}
    \predate{\begin{center}\small}
    \postdate{\end{center}}
  - \usepackage{titlesec}
  - \titleformat{\section}{\normalfont\normalsize\bfseries}{\thesection}{1em}{}
  - \titleformat{\subsection}{\normalfont\normalsize\bfseries}{\thesubsection}{1em}{}
  - \titleformat{\subsubsection}{\normalfont\small\bfseries}{\thesubsubsection}{1em}{}
  - \setlength{\parskip}{1pt}
editor_options:
  markdown:
    wrap: 72
---

# Introduction

The study of A. H. Schaaf, 1966, "Regional Differences in Mortgage
Financing Costs" [@schaaf1966], investigates the existence and causes of regional differences in Mortgage financing costs in the United States.
While these differences in Mortgage Yields were decreasing in the early
20th century, they suprisingly remained stable after World War II. The
paper explores two main explanations for this phenomenon: differences in investment value due to risk, terms, and liquidity, and market
imperfections such as legal barriers and information gaps.\
The dataset, provided by the Federal Home Loan Bank Board, includes interest rates and fees in 18 SMSAs (Standard Metropolitan Statistical Areas). Key findings suggest that distance from major financial centers, local risk levels, and savings demand influence Mortgage Yields more than market structure and overall savings levels.
\vspace{0.5em}

The aim of this report is to analyze the dataset and build a predictive model for Mortgage Yield (in %) using 6 explanatory variables:\
- **X1**: Loan-to-Mortgage Ratio, in % → High values indicate low down
payments.\
- **X2**: Distance from Boston, in miles → Measures regional proximity
to financial centers.\
- **X3**: Savings per New Unit Built, in \$ → Indicator of regional
credit demand.\
- **X4**: Savings per Capita, in \$ → Measures local savings levels
(credit supply).\
- **X5**: Population Increase, 1950-1960, in % → Proxy for housing
demand growth.\
- **X6**: Percentage of First Mortgages from Inter-Regional Banks, in %
→ Indicator of external financing reliance.
\vspace{-0.5em}

# Exploratory Data Analysis (EDA)

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(xtable)
library(MASS)
library(car)
library(magrittr)
library(patchwork)
library(gridExtra)
library(grid)
library(gtable)
library(reshape2)
library(broom)
library(xtable)

# Read data
df <- read.csv("mYield.csv")

# Rename columns for easier reference
colnames(df) <- c("smsa", "mortYld", "X1", "X2", "X3", "X4", "X5", "X6")

# Display first few rows
#kable(head(df), caption = "First few rows of the dataset") %>%
#  kable_styling(font_size = 7, latex_options = "hold_position")
```

```{r, include=FALSE}
# Count missing values
colSums(is.na(df))

```


## Univariate analysis

### Numerical analysis

We begin with a numerical summary of each variable:

\vspace{-0.2cm}
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/Table 1.png}
\captionsetup{font=normalsize}
\caption*{Table 1: Summary Statistics of all Variables. The minimum, 1st quartile, median, mean, 3rd quartile, and maximum are shown for all six variables and the Mortgage Yield.}
\end{figure}
\vspace{-0.5cm}

The summary shows limited regional variation in **mortYld**, with most values between 5.2% and 6.2%, indicating relatively stable mortgage rates.

Loan-to-Mortgage Ratios (**X1**) are concentrated in between 67% and
78.1%. Distance from Boston (**X2**) has a vast range (0–3162 miles),
highlighting geographical diversity and potential financial access
disparities. Savings per New Unit Built (**X3**) and Savings per Capita
(**X4**) are characterized by means bigger than medians, representing
right-skewed distributions. Population Increase (**X5**) from 1950 to
1960 varies widely (7.5–88.9%). Lastly, Percentage of First Mortgages
from Inter-Regional Banks (**X6**) spans from 2.0% to 51.3%, meaning
that some areas depend heavily on external financing while others rely
more on local institutions.

### Graphical analysis
\vspace{-1em}
\begin{figure}[H]
\centering

\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=1.1\linewidth]{figures/Figure 1.png}
\captionsetup{font=normalsize}
\caption*{Figure 1: Histogram of Mortgage Yield across SMSAs (Standard
Metropolitan Statistical Area)}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
\centering
\vspace{-6cm}
\includegraphics[width=1.1\linewidth]{figures/Figure 2.png}
\captionsetup{font=normalsize}
\caption*{Figure 2: Histograms of Mortgage Yield across Predictor Variables}
\end{minipage}

\end{figure}
\vspace{-0.5cm}

Although the variation in Mortgage Yields across SMSAs is limited, regional differences are still observable, likely due to factors such as savings rates, loan characteristics, and local banking practices:

**X1** shows low variance, indicating that the distance from Boston does not vary much across SMSAs.
**X2** has a wide and uniform distribution, reflecting the broad spatial distribution of the regions.
**X3** and **X4** are right-skewed, suggesting that a small number of SMSAs have significantly higher savings levels.
**X5** is also right-skewed, with a potential outlier, indicating uneven population growth.
**X6** shows that most SMSAs have low reliance on external financing, with a few depending heavily on it.

These patterns suggest that regional heterogeneity in economic conditions and credit access contributes to the observed differences in Mortgage Yields.

## Bivariate analysis

### Graphical analysis

The Association Matrix offers a visual overview of pairwise relationships, including form (linear, curved, weak), direction (positive or negative), and outlier presence. It complements the Correlation Matrix.

Most plots show random dispersion: some indicate linear or curved trends. **X3** is positively related to **X4**, and negatively to **X5** and **X6**. **X2** and **X3** show a negative exponential relationship.
\vspace{0.5em}
\begin{minipage}{0.45\textwidth}
\includegraphics[width=1\linewidth]{figures/Figure 3.png}
\small Figure 3: Association Matrix Between Mortgage Yield and Predictors. Displays pairwise correlations, highlighting relationship strength and direction between Mortgage Yield and the 6 variables.
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\vspace{0pt}
Now let's examine the relationship between Mortgage Yield (\%) and each explanatory variable (x-axis), corresponding to the first row of the Association Matrix.\\
\textbf{X1} shows a positive correlation: higher Loan-to-Mortgage Ratios are linked to higher yields.
\textbf{X2} also correlates positively; yields rise with distance from Boston, likely due to reduced capital access.
\textbf{X3} is negatively correlated - regions investing more construction savings tend to have lower yields.
\textbf{X4} shows little to no clear association.
\textbf{X5} has a mildly positive, possibly non-linear relationship, with higher population growth linked to higher yields.
\textbf{X6} shows no clear trend, suggesting limited impact of external financing.

\end{minipage}
\addtocounter{figure}{1}
\vspace{1em}

These observations support the findings of Schaaf (1966) stating that
distance from financial centers, risk factors, and local demand for
savings contribute to Mortgage Yield variations.

### Numerical analysis
\vspace{-1em}
\begin{figure}[H]
\centering
\begin{minipage}{0.49\textwidth}
\vspace{-0.5cm}
The Correlation Matrix confirms earlier patterns.
\textbf{X3} is strongly correlated with \textbf{X4} (0.77) and negatively with \textbf{X2} (–0.64), \textbf{X5} (–0.63), and \textbf{X6} (–0.56).
\textbf{X1} and \textbf{X2} are strongly positively correlated with \textbf{mortYld}, \textbf{X5} and \textbf{X6} moderately, and \textbf{X3} strongly negatively. \textbf{X4} is weakly correlated.  

This suggests potential multicollinearity. Removing one of the highly correlated predictors may improve model stability. Still, low correlation doesn’t rule out non-linear relationships.
\end{minipage}
\hfill
\begin{minipage}{0.49\textwidth}
\vspace{-1em}
\centering
\includegraphics[width=0.82\linewidth]{figures/Figure 5.png}
\captionsetup{font=small}
\caption*{Figure 5: Correlation Heatmap of Mortgage Yield and Predictor Variables. red: strong positive, blue: strong negative, white: no correlation.}
\end{minipage}
\end{figure}


\vspace{-2.5em}

# Model Fitting

All predictors are continuous, and each observation corresponds to a unique SMSA. With no grouping or unequal group sizes, this is a standard multiple linear regression with one observation per row. The order of predictors does not affect the results.

The multiple linear regression model is defined mathematically as:
\vspace{-0.2cm} $$
\text{mortYld}_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i} + \beta_5 X_{5i} + \beta_6 X_{6i} + \varepsilon_i
$$ where $\text{mortYld}_i$ is the mortgage yield for the i-th SMSA, $X_{1i}$ to $X_{6i}$ are the predictors, $\beta_0$ are coefficients, $\beta_1$ to $\beta_6$ are the regression coefficients, and  $\varepsilon_i$ is the error term.

\vspace{0.2cm}

We assume the classical linear regression assumptions:\
1. Linearity: The relationship between each predictor and the outcome is
linear.\
2. Independence: The errors $\varepsilon_i$ are independent across
observations.\
3. Homoscedasticity: The errors have constant variance:
$\text{Var}(\varepsilon_i) = \sigma^2$.\
4. Normality: The errors follow a normal distribution:
$\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$.\
5. No multicollinearity: The predictors are not perfectly linearly
correlated.

\vspace{0.2cm}

The model is fitted using Ordinary Least Squares (OLS), which minimizes
the sum of squared residuals: \(
\min_{\boldsymbol\beta} \sum_{i=1}^{18} \left( \text{mortYld}_i - \beta_0 - \sum_{j=1}^{6} \beta_j X_{ji} \right)^2 \). 

## Null Model vs Full Model Comparison

To test whether the explanatory variables significantly improve the
model fit compared to the intercept-only model, we conduct an ANOVA
comparing the null model and the full (alternative) model. In order to
do so, we test the following hypotheses:

-   **Null Hypothesis (**$H_0$): $\beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0$. i.e. the explanatory variables do not improve the model.

-   **Alternative Hypothesis (**$H_1$): $\text{At least one } \beta_j \neq 0 \quad \text{for } j = 1, \ldots, 6$. i.e. at least one explanatory variable significantly contributes to predicting mortgage yield.

\vspace{-1.2em}
```{r}
null_model <- lm(mortYld ~ 1, data = df)
full_model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)

anova_result <- anova(null_model, full_model)

anova_df <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col)) {
      sapply(col, function(x) {
        if (is.na(x)) {
          NA
        } else if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, anova_result, names(anova_result))
)

anova_df$`Df` <- as.integer(anova_df$`Df`)
anova_df$`Res.Df` <- as.integer(anova_df$`Res.Df`)

anova_df$Model <- c("mortYld ~ 1", "mortYld ~ X1 + X2 + X3 + X4 + X5 + X6")

# Create F-statistic DF column (num, den)
anova_df$`F DF` <- c(NA, paste0(anova_df$Df[2], ",", anova_df$Res.Df[2]))

# Reorganise columns WITHOUT the old 'Df' column
anova_df <- anova_df[, c("Model", "Res.Df", "RSS", "F DF", "Sum.of.Sq", "F", "Pr..F.")]
colnames(anova_df) <- c("Model", "Residual_DF", "RSS", "DF (num,den)", "SS", "F-statistic", "p-value")

# Display with kable
kable(anova_df,
      booktabs = TRUE, digits = 2, format = "latex", escape = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```

\vspace{-0.4em}

\noindent \fontsize{12}{14}\selectfont Table 2: ANOVA Comparison between Null and Full Models. It shows Residual degrees of freedom (Residual_DF), residual sum of squares (RSS), the numerator and denominator degrees of freedom (DF (num, den)), the sum of squares (SS), F-statistic, and p-value to assess whether the full model improves fit. A p-value < 0.05 indicates that at least one predictor is relevant.

\vspace{0.2em}
The ANOVA test confirms that the full model (with predictors) explains Mortgage Yield significantly better than the null model, as shown by the low p-value (p \< 0.001). This means at least one variable contributes meaningfully to the prediction.

```{r}
model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)
model_summary <- broom::tidy(model)

model_summary <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col) && !grepl("^df", name)) {
      sapply(col, function(x) {
        if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else if (abs(x) >= 0.0095) {
          sprintf("%.3f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, model_summary, names(model_summary))
)

colnames(model_summary) <- c("Term", "Estimate", "Std.Error", "F-statistic", "p-value")


# Build table grob
table1 <- tableGrob(model_summary, rows = NULL)

# Apply styling
for (i in seq_along(table1$grobs)) {
  if (is.null(table1$grobs[[i]]$gp)) table1$grobs[[i]]$gp <- gpar()
  table1$grobs[[i]]$gp$fill <- "#ffffff"
  table1$grobs[[i]]$gp$fontsize <- 12
}

# Add border around entire table
table1 <- gtable_add_grob(
  table1,
  rectGrob(gp = gpar(lwd = 1, col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table1), r = ncol(table1)
)

# ---- HEADLESS LAYOUT ----
# Create virtual layout without displaying
gt <- grobTree(table1)

# Open PNG device first (without drawing yet)
png("figures/full_model_summary_table.png", width = 10, height = 10, units = "in", res = 300, bg = "white")

# Force layout and compute exact size
grid.newpage()
grid.draw(gt)

# Compute exact size after layout
w <- convertWidth(sum(table1$widths), "in", valueOnly = TRUE)
h <- convertHeight(sum(table1$heights), "in", valueOnly = TRUE)

# Close temporary oversized file
invisible(dev.off())

# Re-open final PNG device with correct size
png("figures/full_model_summary_table.png", width = w + 0.1, height = h + 0.1, units = "in", res = 300, bg = "white")
grid.newpage()
grid.draw(table1)
invisible(dev.off())


```

\begin{minipage}{0.42\textwidth}
\includegraphics[width=1\linewidth]{figures/full_model_summary_table.png}
\vspace{-0.1em}
{\fontsize{12}{14}\selectfont Table 3: Summary of Full Linear Model}
\end{minipage}
\hfill
\begin{minipage}{0.55\textwidth}
The Full model explains $\sim$87\% of the variance in Mortgage Yield, and 80\% after adjusting for the number of predictors, which highlights a strong fit. The Residual Standard Error is low, and the overall model is statistically significant, with a very low p-value ($p < 0.001$). Once again, it means that at least one term contributes significantly to explaining the variation in \textbf{mortYld}.
\end{minipage}

\vspace{-0.6em}
```{r}
model_stats <- glance(model)[, c("r.squared", "adj.r.squared", "sigma", "statistic", "df", "p.value")]

# Rename columns (LaTeX-ready names)
colnames(model_stats) <- c("$R^2$", "Adjusted\\_$R^{2}$", "Std.Error", "F-statistic", "DF", "p-value")

# Reorder columns
model_stats <- model_stats[, c("Adjusted\\_$R^{2}$", "$R^2$", "Std.Error", "DF", "F-statistic", "p-value")]

# Convert to data.frame
model_stats <- as.data.frame(model_stats)

# Apply rounding
model_stats$`$R^2$` <- round(model_stats$`$R^2$`, 2)
model_stats$`Adjusted\\_$R^{2}$` <- sprintf("%.2f", model_stats$`Adjusted\\_$R^{2}$`)
model_stats$`Std.Error` <- sprintf("%.2f", model_stats$`Std.Error`)
model_stats$`F-statistic` <- round(model_stats$`F-statistic`, 2)
model_stats$`DF` <- as.integer(model_stats$`DF`)
model_stats$`p-value` <- formatC(model_stats$`p-value`, format = "e", digits = 2)

# Display table (optional)
kable(model_stats,
      booktabs = TRUE, format = "latex", escape = FALSE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")
```
\fontsize{12}{14}\selectfont Table 4: Fit Statistics of Full
Linear Model. $R^2$ shows the proportion of variance explained, Adjusted $R^2$ accounts for the number of predictors, and Standard Error measures the average distance of observed values from the fitted values.

\addtocounter{table}{2}

The intercept is highly significant (p < 0.001). Among the predictors, only **X1** and **X3** show weak significance (p $\approx$ 0.05), while **X2**, **X5**, and **X6** are not individually significant.
We begin by fitting the full model to assess both combined and individual effects. Given the limited contribution of several variables, a reduced model may be more appropriate. We proceed with stepwise regression, using statistical significance and AIC as selection criteria.
We end up with : $\hat{mortYld}$ = 4.29 + 0.020$\cdot$**X1** +
1.36$\cdot$$10^{-5}$$\cdot$**X2** - 1.58$\cdot$$10^{-3}$$\cdot$**X3** +
2.02$\cdot$$10^{-4}$$\cdot$**X4** + 1.28$\cdot$$10^{-3}$$\cdot$**X5** +
2.36$\cdot$$10^{-4}$$\cdot$**X6**


## Make stepwise regression to select the best model

```{r save_stepwise_tables, echo=FALSE, message=FALSE, warning=FALSE}
step_trace_df <- data.frame(
  Step = c("Start", "Step 1", "Step 2", "Step 3"),
  Model = c("X1 + X2 + X3 + X4 + X5 + X6",
            "X1 + X2 + X3 + X4 + X5",
            "X1 + X3 + X4 + X5",
            "X1 + X3 + X4"),
  RSS = c(0.1098, 0.1099, 0.1109, 0.1159),
  AIC = c(-77.79, -79.77, -81.61, -82.81)
)

# Format numeric columns with 2 decimals
step_trace_df$RSS <- sprintf("%.2f", step_trace_df$RSS)
step_trace_df$AIC <- sprintf("%.2f", step_trace_df$AIC)

table1 <- tableGrob(step_trace_df, rows = NULL)

# Make "Model" column twice as wide as the others
n <- ncol(table1)
widths_vec <- rep(1, n)
widths_vec[2] <- 2.5  # Adjust this if "Model" is not first column
table1$widths <- unit(widths_vec, "null")

for (i in seq_along(table1$grobs)) {
  if (is.null(table1$grobs[[i]]$gp)) table1$grobs[[i]]$gp <- gpar()
  table1$grobs[[i]]$gp$fontsize <- 11
  table1$grobs[[i]]$gp$fill <- "#ffffff"
}

table1 <- gtable_add_grob(
  table1,
  rectGrob(gp = gpar(lwd = 1, col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table1), r = ncol(table1)
)


# 2. Stepwise Model Coefficients Table
step_model <- stepAIC(model, direction = "both", trace = FALSE)
step_summary <- broom::tidy(step_model)

step_summary <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col) && !grepl("^df", name)) {
      sapply(col, function(x) {
        if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else if (abs(x) >= 0.0095) {
          sprintf("%.3f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, step_summary, names(step_summary))
)

colnames(step_summary) <- c("Term", "Estimate", "Std.Error", "F-statistic", "p-value")

table2 <- tableGrob(step_summary, rows = NULL)
table2$widths <- unit(rep(1, ncol(table2)), "null")

for (i in seq_along(table2$grobs)) {
  if (is.null(table2$grobs[[i]]$gp)) table2$grobs[[i]]$gp <- gpar()
  table2$grobs[[i]]$gp$fontsize <- 12
  table2$grobs[[i]]$gp$fill <- "#ffffff"
}

table2 <- gtable_add_grob(
  table2,
  rectGrob(gp = gpar(lwd = 1, col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table2), r = ncol(table2)
)

# Save as PNGs
ggsave("figures/stepwise_aic_table.png", table1, width = 4.5, height = 1.8, dpi = 300)
ggsave("figures/stepwise_coef_table.png", table2, width = 4.53, height = 1.8, dpi = 300)
```
AIC is the Akaike Information Criterion. A lower AIC indicates a better trade-off between model fit and complexity.

\begin{minipage}{0.48\textwidth}
\vspace{-1.3em}
\centering
\includegraphics[width=\linewidth]{figures/stepwise_aic_table.png}
\parbox{\linewidth}{\fontsize{12}{14}\selectfont Table 5: Stepwise AIC Process.}
\end{minipage}
\hfill
\raisebox{1.6em}{%
\begin{minipage}{0.48\textwidth}
\centering
\vspace{-2pt}
\includegraphics[width=\linewidth]{figures/stepwise_coef_table.png}
\vspace{-2.2em}
\parbox{\linewidth}{\fontsize{12}{14}\selectfont Table 6: Summary of Final Stepwise Model}
\end{minipage}
}
\addtocounter{table}{2}
\vspace{0.3em}

```{r}
# 3. Fit Statistics of Stepwise Model

# Extraire les stats globales
step_glance <- glance(step_model)[, c("r.squared", "adj.r.squared", "sigma", "statistic", "df", "p.value")]
colnames(step_glance) <- c("$R^2$", "Adjusted\\_$R^{2}$", "Std.Error", "F-statistic", "DF", "p-value")

step_glance <- step_glance[, c("Adjusted\\_$R^{2}$", "$R^2$", "Std.Error", "DF", "F-statistic", "p-value")]


# Convertir en data.frame pour mise en forme fine
step_glance <- as.data.frame(step_glance)

step_glance$`$R^2$` <- round(step_glance$`$R^2$`, 2)
step_glance$`Adjusted\\_$R^{2}$` <- sprintf("%.2f", step_glance$`Adjusted\\_$R^{2}$`)
step_glance$`Std.Error` <- sprintf("%.3f", step_glance$`Std.Error`)
step_glance$`F-statistic` <- round(step_glance$`F-statistic`, 2)
step_glance$`DF` <- as.integer(step_glance$`DF`)
step_glance$`p-value` <- formatC(step_glance$`p-value`, format = "e", digits = 2)

# Afficher en tableau LaTeX
kable(step_glance,
      booktabs = TRUE, format = "latex", escape = FALSE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```
\vspace{-0.8em}
\begin{center}
{\fontsize{12}{14}\selectfont Table 7: Fit Statistics of Stepwise Model\par}
\end{center}
\vspace{0.2em}

The Stepwise regression process identifies **X1**, **X3**, and **X4** as the most significant predictors of Mortgage Yield, constituting the
final model. It is interesting to note that **X4** appears among the 3 most significant predictors although it shows very weak correlation in the Correlation Matrix. Multiple regression measures the effect of each variable while holding all others constant. As **X4** has very strong correlation with **X3** (0.77), holding **X3** can make the unique contribution of **X4** clearer.  
\vspace{0em}

The final Stepwise model explains approximately 83.4% of the variance in Mortgage Yield using only these three predictors. The AIC doesn’t increase a lot when keeping more predictors, meaning that even if these predictors can still be statistically valid to keep, they are not so useful to the model. Though the final model is simpler, it explains the data just as well or better than more complex models. The RSE (0.09) is low, and the overall model is highly significant (p < 0.001), indicating a good fit.\
We end up with : $\hat{mortYld}$ = 4.22 + 0.022$\cdot$**X1** -
1.86$\cdot$$10^{-3}$$\cdot$**X3** + 2.25$\cdot$$10^{-4}$$\cdot$**X4**

```{r}
# 1. Coefficients Table
interaction_model <- lm(mortYld ~ X1 + X3 + X4 + X1:X3 + X1:X4 + X3:X4, data = df)
interaction_coef <- broom::tidy(interaction_model)

interaction_coef <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col) && !grepl("^df", name)) {
      sapply(col, function(x) {
        if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else if (abs(x) >= 0.0095) {
          sprintf("%.3f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, interaction_coef, names(interaction_coef))
)

colnames(interaction_coef) <- c("Term", "Estimate", "Std.Error", "F-statistic", "p-value")

# Génération du tableau
table1 <- gridExtra::tableGrob(interaction_coef, rows = NULL)

# Appliquer fond blanc et taille police
for (i in seq_along(table1$grobs)) {
  if (is.null(table1$grobs[[i]]$gp)) table1$grobs[[i]]$gp <- gpar()
  table1$grobs[[i]]$gp$fill <- "#ffffff"
  table1$grobs[[i]]$gp$fontsize <- 12
}

# Cadre noir autour
table1 <- gtable::gtable_add_grob(
  table1,
  grobs = grid::rectGrob(gp = gpar(col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table1), r = ncol(table1)
)

w <- convertWidth(sum(table1$widths), "in", valueOnly = TRUE)
h <- convertHeight(sum(table1$heights), "in", valueOnly = TRUE)

w <- w + 0.01
h <- h + 0.01 + 0.3

# Export PNG ajusté au contenu
png("figures/interaction_model_coef.png", width = w, height = h, units = "in", res = 300, bg = "white")
grid.draw(table1)

invisible(dev.off())


```

\begin{minipage}{0.5\textwidth}
\includegraphics[width=0.92\linewidth]{figures/interaction_model_coef.png}
\vspace{-0.5em}
\fontsize{12}{12}\selectfont Table 8: Summary of 2-way Interaction Model
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
Let's now try a model with 2-way interactions. The 2-way Interaction model, which is more complex than the Stepwise
model, explains approximately 79.9\% of the variance in Mortgage Yield.
The Residual Standard Error (0.10) is low, and the overall model is
highly significant (p < 0.001), indicating that at least one of the
terms has a significant influence on Mortgage Yield.
\end{minipage}

\vspace{2em}
```{r}
interaction_stats <- glance(interaction_model)[, c("r.squared", "adj.r.squared", "sigma", "statistic", "df", "p.value")]

# Rename columns (LaTeX-ready names)
colnames(interaction_stats) <- c("$R^2$", "Adjusted\\_$R^{2}$", "Std.Error", "F-statistic", "DF", "p-value")

# Reorder columns
interaction_stats <- interaction_stats[, c("Adjusted\\_$R^{2}$", "$R^2$", "Std.Error", "DF", "F-statistic", "p-value")]

# Convert to data.frame
interaction_stats <- as.data.frame(interaction_stats)

interaction_stats$`$R^2$` <- round(interaction_stats$`$R^2$`, 2)
interaction_stats$`Adjusted\\_$R^{2}$` <- sprintf("%.2f", interaction_stats$`Adjusted\\_$R^{2}$`)
interaction_stats$`Std.Error` <- sprintf("%.2f", interaction_stats$`Std.Error`)
interaction_stats$`F-statistic` <- round(interaction_stats$`F-statistic`, 2)
interaction_stats$`DF` <- as.integer(interaction_stats$`DF`)
interaction_stats$`p-value` <- formatC(interaction_stats$`p-value`, format = "e", digits = 2)


# Display table (optional)
kable(interaction_stats,
      booktabs = TRUE, format = "latex", escape = FALSE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```
\begin{center}
\vspace{-1em}
{\fontsize{12}{14}\selectfont Table 9: Fit Statistics of 2-way Interaction Model}
\end{center}
\addtocounter{table}{2}
\vspace{0em}

None of the variables show statistically significant individual
contributions: only the intercept appears to be moderately significant
to fit the model (p \< 0.05). This suggests that a reduced model may be
more appropriate.\
We end up with : $\hat{mortYld}$ = 5.37 +
6.91$\cdot$$10^{-3}$$\cdot$**X1** - 1.04$\cdot$$10^{-4}$$\cdot$**X3** -
9.10$\cdot$$10^{-4}$$\cdot$**X4** -
2.09$\cdot$$10^{-5}$$\cdot$**X1**:**X3** +
1.50$\cdot$$10^{-5}$$\cdot$**X1**:**X4** -
4.75$\cdot$$10^{-8}$$\cdot$**X3**:**X4**

We decided not to include a 3-way Interaction model in our analysis.
Given the small sample size (18 observations), adding high-order
interactions would significantly reduce degrees of freedom and increase
the risk of overfitting. Moreover, 3-way interactions are often
difficult to interpret meaningfully.

## Model Comparison

```{r, fig.width=10, fig.height=2}
# Create comparison table
# Create the data.frame with simple names first
model_metrics <- data.frame(
  Model = c("Full Model", "Stepwise Model", "2-Way Interaction Model"),
  Adjusted_R2 = c(
    summary(full_model)$adj.r.squared,
    summary(step_model)$adj.r.squared,
    summary(interaction_model)$adj.r.squared
  ),
  R2 = c(
    summary(full_model)$r.squared,
    summary(step_model)$r.squared,
    summary(interaction_model)$r.squared
  ),
  RSE = c(
    summary(full_model)$sigma,
    summary(step_model)$sigma,
    summary(interaction_model)$sigma
  ),
  AIC = c(
    AIC(full_model),
    AIC(step_model),
    AIC(interaction_model)
  ),
  `F-statistic` = c(
    summary(full_model)$fstatistic[1],
    summary(step_model)$fstatistic[1],
    summary(interaction_model)$fstatistic[1]
  )
)

# Then rename the columns for LaTeX display
colnames(model_metrics) <- c("Model", "Adjusted_R²", "R²", "RSE", "AIC", "F-statistic")

model_metrics$RSE <- c(
  sprintf("%.2f", model_metrics$RSE[1]),
  sprintf("%.3f", model_metrics$RSE[2]),
  sprintf("%.2f", model_metrics$RSE[3])
)


# Display table
kable(model_metrics, digits = 2, escape=FALSE) %>%
  kable_styling(font_size = 8, full_width = FALSE, position = "center")

```

\begin{center}
\vspace{-0.4em}
{\fontsize{12}{14}\selectfont Table 10: Comparison of Model Performance Metrics. RSE (residual standard error) reflects the typical size of prediction errors; lower values indicate better fit.\par}
\end{center}
\vspace{-0.5em}

The Stepwise model offers the best trade-off between simplicity and
performance: it has the lowest AIC (\~29.7), demonstrating the best
model fit among the three. Despite having a slightly lower R² than the
Full and 2-ways Interactions model, it achieves the highest Adjusted R².
It also has the lowest Residual Standard Error (0.09) and the highest
F-statistic (\~29.5). This confirms the overall model significance and
parsimony.

```{r}
# Compare with ANOVA and convert to data frame
anova_result <- anova(step_model, interaction_model)

# Format numeric values: if ≥ 0.095 show 2 decimals, else use scientific notation
anova_df <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col)) {
      sapply(col, function(x) {
        if (is.na(x)) {
          NA
        } else if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, anova_result, names(anova_result))
)

anova_df$`Df` <- as.integer(anova_df$`Df`)
anova_df$`Res.Df` <- as.integer(anova_df$`Res.Df`)
anova_df$Model <- c("Stepwise model", "Interaction model")

# Réorganiser les colonnes
anova_df <- anova_df[, c("Model", "Res.Df", "RSS", "Df", "Sum.of.Sq", "F", "Pr..F.")]
colnames(anova_df) <- c("Model", "Residual_DF", "RSS", "DF", "SS", "F-statistic", "p-value")

# Affichage avec kable
kable(anova_df,
      booktabs = TRUE, digits = 2, format = "latex", escape = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```
\begin{center}
\vspace{-1em}
{\fontsize{12}{14}\selectfont Table 11: ANOVA Comparison Between Stepwise and Interaction Models\par}
\end{center}

Since the Stepwise Model shows the lowest AIC, we perform an ANOVA to verify whether the added complexity of the Interaction Model provides a significant improvement. The Interaction Model is a nested extension of the Stepwise Model, including additional two-way interaction terms. The test yields an F-statistic of 0.18 and a p-value of 0.91, indicating
that the additional interaction terms do not significantly reduce the
residual variance. As a result, the simpler model with only main effects
(**X1**, **X3**, and **X4**) truly provides the best fit, as it also
offers comparable explanatory power and better interpretability.
\vspace{-1.1em}

# Model assumptions and Diagnostics

In order to trust the results of our regression model, we must ensure
that the residuals satisfy the following assumptions, evaluated using residual diagnostic plots:

1.  The residuals have an expected value (mean) of 0:
    $\mathbb{E}[\varepsilon_i] = 0$
2.  The residuals are homoscedastic (have constant variance):
    $\text{Var}(\varepsilon_i) = \sigma^2$
3.  The residuals are uncorrelated:
    $\text{Cov}(\varepsilon_i, \varepsilon_j) = 0$ for $i \neq j$
4.  The residuals are normally distributed:
    $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$

## Independence evaluation

```{r, fig.width=7.5, fig.height=2.5}
par(mfrow = c(1, 3),        
    mar = c(4, 4, 1, 1),       # bottom margin increased to fit caption
    mgp = c(2, 0.6, 0),     
    cex.main = 1.2,     # Title size (not used now)
    cex.lab = 1.2,      
    cex.axis = 1.0)

# Residuals vs. Fitted Values Plot (no top title)
plot(fitted(model), resid(model),
     main = "",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 19, col = "blue")
res_fit_lm <- lm(resid(model) ~ fitted(model))
abline(res_fit_lm, col = "red", lwd = 2)
coef_res_fit <- coef(res_fit_lm)
text(min(fitted(model)), max(resid(model)) * 0.8, 
     labels = paste0("y = ", formatC(coef_res_fit[2], format = "e", digits=2), "\u00B7x\n+ ", formatC(coef_res_fit[1], format = "e", digits=2)), 
     pos = 4, col = "red", cex = 1.2)

# Residuals vs. Observation Order Plot (no top title)
plot(resid(model), type = "p",
     main = "",
     xlab = "Observation Index", ylab = "Residuals",
     pch = 19, col = "darkgreen")
res_obs_lm <- lm(resid(model) ~ seq_along(resid(model)))
abline(res_obs_lm, col = "red", lwd = 2)
coef_res_obs <- coef(res_obs_lm)
text(1, max(resid(model)) * 0.9, 
     labels = paste0("y = ", formatC(coef_res_obs[2], format = "e", digits=2), "\u00B7x + ", sprintf("%.3f", coef_res_obs[1])), 
     pos = 4, col = "red", cex = 1.2)

# Scale-Location Plot (no top title)
plot(fitted(model), sqrt(abs(resid(model))),
     main = "",
     xlab = "Fitted Values", ylab = expression(sqrt(abs(Residuals))),
     pch = 19, col = "purple")
scale_loc_lm <- lm(sqrt(abs(resid(model))) ~ fitted(model))
abline(scale_loc_lm, col = "red", lwd = 2)
coef_scale_loc <- coef(scale_loc_lm)
text(min(fitted(model)), max(sqrt(abs(resid(model)))) * 0.95,
     labels = paste0("y = ", round(coef_scale_loc[2], 2), "\u00B7x + ", round(coef_scale_loc[1], 2)), 
     pos = 4, col = "red", cex = 1.2)


# Reset plotting parameters
par(mfrow = c(1, 1))


```

\begin{figure}[htbp]
\vspace{-1em}
  \centering
  \begin{minipage}[b]{0.3\linewidth}
    {\fontsize{12}{14}\selectfont \small Figure 6: Residuals vs Fitted Plot. It displays residual spread to assess homoscedasticity.\par}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.3\linewidth}
    {\fontsize{12}{14}\selectfont \small Figure 7: Residuals vs Observation Order Plot. It shows residuals over SMSAs to detect trends or autocorrelation.\par}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.3\linewidth}
    {\fontsize{12}{14}\selectfont \small Figure 8: Scale-Location Plot. It shows the variance of residuals versus fitted values to check for homoscedasticity.\par}
  \end{minipage}
\end{figure}

The `Residuals VS Fitted Values` plot checks for linearity and constant variance. Ideally, residuals should be symmetrically scattered around zero with no clear pattern or funnel shape (indicates constant variance). In our case, the residuals are randomly dispersed with no visible trend (randomness suggests independence), and the red regression line is nearly flat (slope = \(-1.05 \times 10^{-16}\)), suggesting that the residuals have approximately zero mean and constant variance. Therefore, assumptions 1 and 2 are satisfied.

The `Residuals VS Observation Order` is used to detect autocorrelation in the residuals (assumption 3). A patternless distribution across observations indicates independence. The red regression line has a slope of \(-0.0042\), which is close to zero, and residuals appear randomly scattered, suggesting that residuals are not autocorrelated. Hence, assumption 3 appears is verified.

Lastly, the `Scale-Location` plot, shows the square root of standardized residuals vs fitted values. A line with constant spread indicates constant variance : although the slope is somewhat negative (\(-0.29\)), the spread remains relatively even. There is no clear increasing or decreasing funnel shape. This is a sign that our model doesn't suffer from heteroscedasticity and is likely a good fit: this supports the homoscedasticity assumption.

In conclusion, based on Figures 6–8, we find that the residuals have a mean close to zero, appear homoscedastic, and show no sign of autocorrelation. Therefore, assumptions 1, 2, and 3 are reasonably satisfied.


```{r save_vif_table, echo=FALSE, message=FALSE, warning=FALSE}
# Compute VIF
vif_values <- vif(step_model)
vif_df <- data.frame(
  Variable = rownames(as.data.frame(vif_values)),
  VIF = round(as.numeric(vif_values), 2)
)

# Create grob table with white background and black borders
vif_table <- tableGrob(vif_df, rows = NULL)

# Equalize column widths
vif_table$widths <- unit(rep(1, ncol(vif_table)), "null")

# White background + font size
for (i in seq_along(vif_table$grobs)) {
  if (is.null(vif_table$grobs[[i]]$gp)) vif_table$grobs[[i]]$gp <- gpar()
  vif_table$grobs[[i]]$gp$fill <- "#ffffff"
  vif_table$grobs[[i]]$gp$fontsize <- 12
}

# Add black border
vif_table <- gtable_add_grob(
  vif_table,
  rectGrob(gp = gpar(col = "black", fill = NA, lwd = 1)),
  t = 1, l = 1, b = nrow(vif_table), r = ncol(vif_table)
)

# Save as PNG
ggsave("figures/vif_table.png", vif_table, width = 1.5, height = 1.3, dpi = 300)

```

## Multicolinearity diagnostic

\vspace{-0.5em}
\begin{minipage}{0.7\textwidth}
\vspace{0.3cm}
Variance Inflation Factors (VIF) for all final model variables are below 5 (see Table 12), suggesting no problematic multicollinearity.
Though \textbf{X3} and \textbf{X4} have a correlation of 0.77, their VIFs (4.55 and 3.35) are within acceptable limits. Thus, they provide distinct and valuable information.\\
In conclusion, there is no evidence of problematic multicollinearity, and all explanatory variables contribute distinct information to the model.
\end{minipage}
\hfill
\begin{minipage}{0.27\textwidth}
\vspace{-3.7em}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/vif_table.png}
    \vspace{-0.5em}
    \captionsetup{font=normalsize}
    \caption*{Table 12: Variance Inflation Factors (VIF). Values > 5: potential multicollinearity. Values > 10: strong multicollinearity.}
  \end{figure}
\end{minipage}

## Normality Check

```{r save_qqplot, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

png("figures/qqplot_residuals.png", width = 650, height = 650, res = 150)

par(pty = "s", 
    mar = c(4, 3.5, 2, 1),
    mgp = c(2, 0.5, 0),
    cex.main = 1.6,
    cex.lab = 1.6,
    cex.axis = 1.4)

qqnorm(resid(step_model), 
       main = "", 
       xlab = "Theoretical Quantiles", 
       ylab = "Sample Quantiles",
       pch = 19, col = "steelblue",
       xlim = c(-2, 2), ylim = c(-0.2, 0.2))
qqline(resid(step_model), col = "red")

dev.off()

```

\noindent

\begin{minipage}{0.60\textwidth}
\justifying
The \texttt{Q-Q plot} checks if residuals are normally distributed (assumption 4). Most points align with the 45° line, especially in the center, indicating approximate normality.
Some deviation at the tails suggests potential outliers or heavy tails, but with only 18 observations, this is not a strong concern.\\
Overall, normality is reasonably satisfied.
With only 18 observations, such deviations can be expected and are not strong evidence against normality.
\end{minipage}
\hfill
\begin{minipage}{0.38\textwidth}
\centering
\vspace{-2.5em}  % Reduce space before image
\includegraphics[width=0.7\linewidth]{figures/qqplot_residuals.png}
\vspace{-1.6em}
\parbox{\linewidth}{\fontsize{12}{14}\selectfont Figure 9: Q-Q Plot of Residuals.}
\end{minipage}
\vspace{0.5em}
In conclusion, the residuals appear to be approximately normally distributed, and assumption 4 is reasonably met.

# Conclusion
First, EDA revealed regional variation in mortgage yields linked to housing finance, savings, and credit conditions.
Assuming linear regression conditions, we compared the Null and Full models; the Full model showed a significantly better fit.
Stepwise regression identified **X1**, **X3**, and **X4** as key predictors.
We then tested a two-way Interaction model between these variables.
Among the three models, the Stepwise model had the lowest AIC.
An ANOVA confirmed that the Stepwise model (as a reduced version of the Interaction model) remains statistically more significant.\
The final model is thus: $\hat{mortYld}$ = 4.22 +
0.022$\cdot$**X1** - 1.86$\cdot$$10^{-3}$$\cdot$**X3** +
2.25$\cdot$$10^{-4}$$\cdot$**X4**,\
with **X1**: Loan-to-Mortgage Ratio, **X3**: Savings per
New Unit Built, and **X4**: Savings per Capita.
\vspace{0.5em}

Overall, our analysis shows that higher **X1** increases yield, suggesting greater returns from higher borrowing. Higher **X3** lowers yield, reflecting reduced mortgage reliance. **X4** has a mild positive effect, possibly tied to local financial stability.\
The final stepwise regression model explains approximately 83.4% of the variance in Mortgage Yield, indicating a strong fit. Residual diagnostics suggest that linearity, independence, and homoscedasticity assumptions are generally satisfied, though some minor deviations remain.

Mortgage Yield is thus mainly influenced by financial leverage and local savings levels, consistent with economic theory. Future work could explore more predictors, non-linear trends, or robust regression to address residual issues [@wilcox2004robust].
\vspace{0.5em}

In conclusion, the model provides valuable understanding of the factors influencing Mortgage Yield across regions and offers a solid foundation for further predictive or policy analysis.

# References
