---
title: "Predicting Mortgage Yield using Regression Analysis"
author: "Group 42"
date: "`r Sys.Date()`"
output: pdf_document
geometry: margin=2.5cm
fontsize: 12pt
header-includes:
  - \usepackage{graphicx}
  - \usepackage{amsmath}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{fancyhdr}
  - \usepackage{ragged2e}
  - \justifying
  - \pagestyle{fancy}
  - \fancyhead[L]{Clara Delandre, Majandra Garcia, Paola Biocchi, Coline Leteurte}
  - \fancyfoot[C]{\thepage}
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction

The study of A. H. Schaaf, 1966, "Regional Differences in Mortgage
Financing Costs" investigates the existence and causes of regional
differences in mortgage financing costs in the United States. These
differences in mortgage yields have decreased in the early 20th century,
however they remained stable after World War 2. The paper explores two
main explanations for this phenomenon:\
\
**1.** differences in investment value due to risk, terms, and
liquidity\
**2.** market imperfections such as legal barriers and information
gaps.\
\
The data used in this study comes from the Federal Home Loan Bank Board,
which contains interest rates and fees in 18 SMSAs (Standard
Metropolitan Statistical Areas). The findings suggest that distance from
major financial centers, risk levels, and local demand for savings
significantly affect mortgage yields. However, market structure and
overall savings levels play a lesser role.\
\
The aim of this report is to analyze the data and develop a predictive
model to predict Mortgage Yield (`mortYld`) based on 8 explanatory
variables:\
\
- **smsa:** Standard Metropolitan Statistical Areas (18) → Name of the
city/region.\
- **mortYld:** Mortgage Yield, in % → The percentage return on a
mortgage.\
- **X1:** Loan-to-Mortgage Ratio, in % → High values indicate low down
payments.\
- **X2:** Distance from Boston, in miles → Measures regional proximity
to financial centers.\
- **X3:** Savings per new unit built, in \$ → Indicator of regional
credit demand.\
- **X4:** Savings per capita, in \$ → Measures local savings levels
(credit supply).\
- **X5:** Population increase, 1950-1960, in % → Proxy for housing
demand growth.\
- **X6:** Percentage of first mortgages from inter-regional banks, in %
→ Indicator of external financing reliance.\

------------------------------------------------------------------------

# Exploratory Data Analysis (EDA)

## Load Data and Libraries

```{r, echo=FALSE}
# Load necessary libraries
library(ggplot2)
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(xtable)
library(MASS)
library(car)
library(magrittr)

# Read data
df <- read.csv("myield.csv")

# Rename columns for easier reference
colnames(df) <- c("smsa", "mortYld", "X1", "X2", "X3", "X4", "X5", "X6")

# Display first few rows
kable(head(df), caption = "First few rows of the dataset") %>% kable_styling()
```

```{r}
# Count missing values
colSums(is.na(df))

```

Here is a display of the data, on the first few rows of the dataset. It
contains mortgage yield (mortYld) as the dependent variable and six
variables (X1 to X6). smsa represents the Standard Metropolitan
Statistical Area which is the name of the city/region. We can observe
that all data are numerical values and there is no missing value for
each region.

## Univariate Analysis

### Summary Statistics

```{r}
# Summary Statistics
summary_stats <- summary(df[, colnames(df) != "smsa"])
kable(summary_stats, caption = "Summary Statistics of Variables") %>% kable_styling()
```

We can observe that each variable 18 observations corresponding to 18
different SMSAs. Through this summary, we can already observe mortgage
yields don’t vary much across regions. Most values are between 5.5% and
6.2%, suggesting relatively stable mortgage rates.

Loan-to-mortgage ratios (X1) are concentrated in between 70% and 78%,
with low variance. Savings per New Unit Built (X3) are characterized by
a mean bigger than the median, representing a right-skewed distribution
and then large disparities in housing affordability across regions.

-\> to complete

### Graphical Representation

```{r}
# Histogram of Mortgage Yield
ggplot(df, aes(x = mortYld)) + 
  geom_histogram(binwidth = 0.2, fill = "blue", alpha = 0.6) +
  labs(title = "Distribution of Mortgage Yield", x = "Mortgage Yield (%)", y = "Count")

# Create a named vector where mortgage yield values are associated with their respective SMSAs
df <- df %>% arrange(desc(mortYld))  

# Create a better bar chart
ggplot(df, aes(x = reorder(smsa, mortYld), y = mortYld)) + 
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +  # Flip coordinates to make labels readable
  labs(title = "Mortgage Yield by SMSA", x = "SMSA (City)", y = "Mortgage Yield (%)") +
  theme_minimal()


# Set up a 2x3 grid for multiple histograms
par(mfrow = c(2, 3))

# Define variable names
var_names <- c("Loan-to-Mortgage Ratio", "Distance from Boston", 
               "Savings per New Unit Built", "Savings per Capita", 
               "Population Increase", "First Mortgage from Inter-Regional Banks")

# Define colors for better visualization
hist_colors <- c("red", "blue", "green", "purple", "orange", "cyan")

# Loop through each variable (X1 to X6) and create a histogram
for (i in 1:6) {
  hist(df[[paste0("X", i)]], 
       main = paste("Histogram of", var_names[i]),
       xlab = var_names[i], 
       col = hist_colors[i], 
       border = "black")
}

# Reset plotting parameters to default
par(mfrow = c(1, 1))

```

1st graph : doesn't seem to follows a normal distribution.

2nd graph : There is not a huge variation in mortgage yield across
SMSAs, as most bars are at similar heightsbut if we focus on 4 and 6%,
we see regional differences exist in mortgage yields, possibly due to
economic factors like savings, loan terms, and regional banking
practices.

3rd graph :

The histograms reveal the characteristics of the predictor variables.

The loan-to-mortgage ratio (X1) shows low variance with most values
concentrated between 70% and 78%, indicating limited variability across
regions. It might suggest that this variable has limited predictive
power in explaining mortgage yield variation.

Distance from Boston (X2) displays a wide and multimodal distribution,
reflecting substantial geographic spread among SMSAs.

Savings per new unit built (X3) and savings per capita (X4) both exhibit
right-skewed distributions, suggesting that a few cities have notably
higher savings levels.

Population increase (X5) is highly skewed with one major outlier,
indicating that most cities had moderate growth, while a few experienced
rapid expansion. We can also observe potential outliers.

Finally, the percentage of first mortgages from inter-regional banks
(X6) is also right-skewed, with most cities relying minimally on
external financing and a few showing heavy dependence.

These patterns suggest that certain variables may benefit from
transformation prior to regression modeling.

## Bivariate Numerical Analysis

### Association Analysis

```{r}
# Adjust the plotting window size to be square-like (you may need to resize RStudio plot panel)
par(mfrow = c(1, 1))  # Reset to 1x1 layout
par(mai = c(1, 1, 1, 1))  # Adjust margins to give more space around the plot
par(pin = c(10, 10))  # Set plot aspect ratio manually (width and height in inches)

# Select relevant numerical variables
selected_vars <- df[, c("mortYld", "X1", "X2", "X3", "X4", "X5", "X6")]

# Create the scatter plot matrix
pairs(selected_vars, 
      main = "Association Matrix of Variables and mortYld",
      col = "blue",   # Set color for points
      pch = 19)       # Use solid dots for points


```

The scatterplot matrix provides a quick visual assessment of linearity,
strength of associations, potential collinearity among predictors, and
outlier detection. It complements numerical analyses like the
correlation matrix and VIF.

We can visualizes bivariate relationships, **how each variable relates
to the others**, especially to mortYld and assess if a relationship is
linear, curved, or weak, positive or negative. We can also spot
**outliers** or cities that don’t follow the general trend.

We can see that most of the plots are random dispersion, some are
linear, and some are curved. X3 seems to be positively associated with
X4 and negatively with X5. X2:X3 seems more exponential, mortYld seems
to be square root associated with X5.

Let's take a closer look into the Association Matrix, regarding the
relationship between Mortgage Yield (%) and the explanatory variables
(x-axis).

```{r}
# Set up a 2x3 grid for multiple scatter plots
par(mfrow = c(2, 3))

# Define variable names
var_names <- c("Loan-to-Mortgage Ratio", "Distance from Boston", 
               "Savings per New Unit Built", "Savings per Capita", 
               "Population Increase", "First Mortgage from Inter-Regional Banks")

# Define colors for better visualization
point_colors <- c("red", "blue", "green", "purple", "orange", "cyan")

# Loop through each variable (X1 to X6) and create a scatter plot with mortYld on the Y-axis
for (i in 1:6) {
  plot(df[[paste0("X", i)]], df$mortYld,
       main = paste(var_names[i], "vs Mortgage Yield"),
       xlab = var_names[i], ylab = "Mortgage Yield (%)",
       col = point_colors[i], pch = 19)
}

# Reset plotting parameters to default
par(mfrow = c(1, 1))

```

**1. Loan-to-Mortgage Ratio:** As this ratio increases, the Mortgage
Yield increases. This suggests a positive correlation, and that higher
loan-to-mortgage ratios (more borrowed money relative to the property
value) are associated with higher mortgage yields. This is likely due to
increased risk for lenders when down payments are low.\
**2. Distance from Boston:** There is a positive correlation. Boston
represents a major financial center with surplus capital. Regions
further from Boston might have higher yields due to higher transfer
costs or credit shortages.\
**3. Savings per New Unit Built :** There is a negative correlation.
This indicates that areas with more savings dedicated to new
construction have better access to local financing, resulting in lower
mortgage yields. It reflects regional credit demand and its influence on
yield determination.\
**4. Savings per Capita:** The relationship is less clear but appears to
be a weak negative correlation. This suggesting that greater local
savings levels (credit supply) may reduce the need for external
financing. However, the impact is not as strong as the previous plots.\
**5. Population Increase:** There is a positive correlation. High
population growth may imply higher demand for housing, increasing
mortgage yields due to heightened competition for available funds.
Reflects regional economic growth and housing demand.\
**6. First Mortgage from Inter-Regional Banks:** No clear trend. It
seems like the reliance on external financing (measured by the
percentage of first mortgages from inter-regional banks) does not
significantly influence mortgage yields. The absence of a trend could
indicate that other factors, such as local market structures and
policies, play a more prominent role.\
\
**To resume:**\
- Loan-to-Mortgage Ratio (X1), Distance from Boston (X2), and Population
Increase (X5) are the most influential variables positively correlated
with Mortgage Yield.\
- Savings per New Unit Built (X3) is the most influential negative
variable, indicating that local savings availability can effectively
reduce yields.\
- The Savings per Capita (X4) and External Financing (X6) variables show
weak relationships with mortgage yields.\
- These observations support the findings of Schaaf (1966) that distance
from financial centers, risk factors, and local demand for savings
contribute to yield variations.\

### Correlation analysis

```{r}
# Correlation matrix
cor_matrix <- cor(df[, c("mortYld", "X1", "X2", "X3", "X4", "X5", "X6")])

# Save the correlation matrix as a data frame for display
cor_df <- as.data.frame(round(cor_matrix, 2))


# Table
kable(cor_df, caption = "Correlation Matrix (Variables: mortYld to X6)", 
      align = 'c', booktabs = TRUE) %>%
  kable_styling(full_width = FALSE, position = "left") %>%
  row_spec(0, bold = TRUE)

# Heatmap
library(reshape2)
melted_cor <- melt(cor_matrix)

ggplot(data = melted_cor, aes(x=Var1, y=factor(Var2, levels=rev(unique(Var2))), fill=value)) + 
  geom_tile() +
  scale_fill_gradient2(low="blue", high="red", mid="white", 
                       midpoint=0, limit=c(-1,1), space="Lab") +
  scale_x_discrete(position = "top") +
  coord_fixed() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 0)) +
  ggtitle("Correlation Heatmap")

```

```{r}
cor(residuals(lm(df$X2 ~ df$X3)), residuals(lm(df$X6 ~ df$X3)))

```

We see that X2 is only weakly positively correlated (r = 0.21) to X6
after controlling for X3; compare this to the much higher simple
correlation (r = 0.60). In other words, much of the apparent correlation
between X2 and X6 can be explained by their mutual positive correlation
with X3.

Numerical interpretation : LINEAR RELASHIONSHIPS

X3 (Savings per New Unit Built) and X4 (Savings per Capita) seem to be
strongly correlated (0.77). X2 (Distance from Boston) and X3 (Savings
per New Unit Built) have a high negative correlation (-0.64). X3
(Savings per New Unit Built) is also negatively correlated with X5
(Population Growth) at -0.63.

We can then think about removing one of the highly correlated
predictors, if multicollinearity affects the regression model.

numerical value of r : curve ? outliers? parallel lines?

------------------------------------------------------------------------

# Model Fitting

## Pairwise Simple Regressions

```{r}
# Pairwise Simple Regressions
models_simple <- lapply(1:6, function(i) {
  formula <- as.formula(paste("mortYld ~ X", i, sep = ""))
  lm(formula, data = df)
})

simple_summaries <- lapply(models_simple, summary)

data.frame(
  Predictor = paste0("X", 1:6),
  R_squared = sapply(simple_summaries, function(m) round(m$r.squared, 3)),
  p_value = sapply(simple_summaries, function(m) round(coef(m)[2,4], 4))
) %>%
  kable(caption = "Simple Linear Regressions: R² and p-values") %>%
  kable_styling()
```

## Null Model vs Full Model Comparison

```{r}
# Null Model vs Full Model Comparison
null_model <- lm(mortYld ~ 1, data = df)
full_model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)

anova(null_model, full_model)
```

Here we can see that the Null model don't fit the data. We really should
the variables.

```{r}
# Fit linear regression model
model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)

# Model summary
summary(model)

```

## Make stepwise regression to select the best model

By removing some of the variables

```{r}
# Stepwise regression (both directions)
step_model <- stepAIC(model, direction = "both")
summary(step_model)
```

The stepwise regression process identified X1, X3, and X4 as the most
significant predictors of mortality yield, leading to the final model.:\
The AIC isn't increased by a lot when keeping the other variables, which
means that these are still statistically valid but not so useful.:\
mortYld = 4.223 + 0.02229.X1 - 0.001863.X3 + 0.0002249.X4\
Now we will test a model with 3-way interactions.

```{r}
#model with interactions
interaction_model <- lm(mortYld ~ X1 * X3 * X4, data = df)

# Summary of the model
summary(interaction_model)

```

The model complexity increased, but there was no significant improvement
in performance.\
Let's try a model with only 2-way interactions, and use a AIC step-wise
selection to select only relevant interactions

```{r}
reduced_interaction_model <- lm(mortYld ~ X1 + X3 + X4 + X1:X3 + X1:X4 + X3:X4, data = df)
summary(reduced_interaction_model)
step_interaction_model <- stepAIC(lm(mortYld ~ X1 * X3 * X4, data = df), direction = "both")
summary(step_interaction_model)

```

This model is worse then with all 3-way interactions.

## Model Comparison

```{r}
# Compare model performances
adj_r2_compare <- data.frame(
  Model = c("Full Model", "Stepwise Model", "2-Way Interaction Model"),
  Adj_R2 = c(
    summary(full_model)$adj.r.squared,
    summary(step_model)$adj.r.squared,
    summary(reduced_interaction_model)$adj.r.squared
  )
)

kable(adj_r2_compare, caption = "Adjusted R² for Different Models") %>% kable_styling()

# Compare AIC
aic_compare <- AIC(full_model, step_model, reduced_interaction_model)
kable(aic_compare, caption = "AIC for Different Models") %>% kable_styling()

# Compare with ANOVA
anova(full_model, step_model, reduced_interaction_model)
```

------------------------------------------------------------------------

# Model assumptions and Diagnostics

## Multicolinearity Check using VIF

a VIF \> 5 indicates possible multicollinearity that can be problematic

:   we are not able to identify the variables associated with this
    collinearity but we will have to do it next.

```{r}
# Compute Variance Inflation Factor (VIF)
vif_values <- vif(model)
kable(as.data.frame(vif_values), caption = "Variance Inflation Factors (VIF)") %>% kable_styling()

```

## Residuals Analysis

Random scatter indicates good assumption of homeoscedasticity. If we can
distinguish a clear pattern, then we have potential heteroscedasticity
issue.

```{r}
# Residuals vs Fitted Plot
ggplot(data.frame(fitted = fitted(model), residuals = resid(model)), aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs Fitted Plot", x = "Fitted Values", y = "Residuals")
```

## Normality Check

If points lie on 45 degrees line, it means the residuals are normally
distributed. If we can see a curved pattern, then the normality
assumption is violated

```{r}
# QQ-Plot of Residuals
residuals <- resid(model)  # Extract residuals

# Set up a square plotting area
par(pty = "s")  

# Generate the Q-Q plot
qqnorm(residuals, main = "Q-Q Plot of Residuals") #Normal Q-Q Plot
qqline(residuals, col = "red")

# Reset plotting parameters (optional)
par(pty = "m")  
```

------------------------------------------------------------------------

# Final estimated Model

```{r}

```

------------------------------------------------------------------------

# Conclusions

```         
•   The analysis showed that [mention significant predictors] have a strong relationship with mortgage yield.
•   The assumptions of linear regression were [state if met or violated].
•   The model provides [good/poor] predictive accuracy based on [R² and residual analysis].
•   Future improvements could involve [mention possible improvements like transformations, additional predictors, etc.].
```
