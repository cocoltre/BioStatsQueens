---
title: '**Predicting Mortgage Yield using Regression Analysis**'
author: 'Group 42: Clara Delandre, Majandra Garcia, Paola Biocchi, Coline Leteurtre'
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
    toc: false
    latex_engine: pdflatex
    keep_tex: true
  html_document:
    toc: false
    df_print: paged
bibliography: refs.bib
geometry: top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm
fontsize: 12pt
header-includes:
- \usepackage{graphicx}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage[font=footnotesize,skip=4pt]{caption}
- \usepackage{caption}
- \usepackage{fancyhdr}
- \usepackage{ragged2e}
- \usepackage{multicol}
- \justifying
- \pagestyle{fancy}
- \fancyhead[L]{Delandre, Garcia, Biocchi, Leteurtre}
- \fancyfoot[C]{\thepage}
- \usepackage{titlesec}
- \titlespacing*{\title}{0pt}{0pt}{0pt}
- "\\titlespacing*{\\author}{0pt}{-0.5cm}{0pt}"
- "\\titlespacing*{\\date}{0pt}{-0.5cm}{0pt}"
- \titlespacing*{\section}{0pt}{0.5cm}{0pt}
- \titlespacing*{\subsection}{0pt}{0.5cm}{0pt}
- \titlespacing*{\subsubsection}{0pt}{0.8cm}{0pt}
- \usepackage{etoolbox}
- \patchcmd{\maketitle}{\vspace*{2\baselineskip}}{}{}{}
- \usepackage{titling}
- "\\setlength{\\droptitle}{-1.5cm}"
- \setlength{\headheight}{12pt}
- \setlength{\headsep}{5pt}
- \setlength{\parindent}{0pt}
- \setlength{\parskip}{1pt}
editor_options:
  markdown:
    wrap: 72
---

# Introduction

The study of A. H. Schaaf, 1966, "Regional Differences in Mortgage
Financing Costs" [@schaaf1966], investigates the existence and causes of
regional differences in Mortgage financing costs in the United States.
While these differences in Mortgage Yields were decreasing in the early
20th century, they suprisingly remained stable after World War II. The
paper explores two main explanations for this phenomenon: differences in
investment value due to risk, terms, and liquidity, and market
imperfections such as legal barriers and information gaps.
\vspace{0.5pt} The data used in this study comes from the Federal Home
Loan Bank Board, which contains interest rates and fees in 18 SMSAs
(Standard Metropolitan Statistical Areas). The findings suggest that
distance from major financial centers, risk levels, and local demand for
savings significantly affect Mortgage Yields. However, market structure
and overall savings levels play a lesser important role. \vspace{0.5pt}
The aim of this report is to analyze the data and develop a model to
predict Mortgage Yield (in %) based on 6 explanatory variables:\
- **X1**: Loan-to-Mortgage Ratio, in % → High values indicate low down
payments.\
- **X2**: Distance from Boston, in miles → Measures regional proximity
to financial centers.\
- **X3**: Savings per New Unit Built, in \$ → Indicator of regional
credit demand.\
- **X4**: Savings per Capita, in \$ → Measures local savings levels
(credit supply).\
- **X5**: Population Increase, 1950-1960, in % → Proxy for housing
demand growth.\
- **X6**: Percentage of First Mortgages from Inter-Regional Banks, in %
→ Indicator of external financing reliance.

# Exploratory Data Analysis (EDA)

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(xtable)
library(MASS)
library(car)
library(magrittr)
library(patchwork)
library(gridExtra)
library(grid)
library(gtable)
library(reshape2)
library(broom)
library(xtable)

# Read data
df <- read.csv("mYield.csv")

# Rename columns for easier reference
colnames(df) <- c("smsa", "mortYld", "X1", "X2", "X3", "X4", "X5", "X6")

# Display first few rows
#kable(head(df), caption = "First few rows of the dataset") %>%
#  kable_styling(font_size = 7, latex_options = "hold_position")
```

```{r, include=FALSE}
# Count missing values
colSums(is.na(df))

```

Each SMSA in the dataset is described by its Mortgage Yield as the
dependent variable, along with six explanatory variables (X1 to X6).
These variables include financial ratios, regional distances, savings
indicators, population growth, and bank origination shares. All
variables are numerical, and a preliminary check confirms there are no
missing values in any of the observations.

## Univariate analysis

### Numerical analysis

We begin with a numerical summary of each variable:

\vspace{-0.5cm}
\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/Table 1.png}
\captionsetup{font=normalsize}
\caption*{Table 1: Summary Statistics of all Variables}
\end{figure}
\vspace{-0.5cm}

Through this summary, we already observe that Mortgage Yields
(**mortYld**) don’t vary much across regions. Most values are between
5.2% and 6.2%, suggesting relatively stable Mortgage rates.

Loan-to-Mortgage Ratios (**X1**) are concentrated in between 67% and
78.1%. Distance from Boston (**X2**) has a vast range (0–3162 miles),
highlighting geographical diversity and potential financial access
disparities. Savings per New Unit Built (**X3**) and Savings per Capita
(**X4**) are characterized by means bigger than medians, representing
right-skewed distributions. Population Increase (**X5**) from 1950 to
1960 varies widely (7.5–88.9%). Lastly, Percentage of First Mortgages
from Inter-Regional Banks (**X6**) spans from 2.0% to 51.3%, meaning
that some areas depend heavily on external financing while others rely
more on local institutions.

### Graphical analysis

\begin{figure}[H]
\centering

\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=1.1\linewidth]{figures/Figure 1.png}
\captionsetup{font=normalsize}
\caption*{Figure 1: Histogram of Mortgage Yield across SMSAs (Standard
Metropolitan Statistical Area)}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\textwidth}
\centering
\vspace{-6cm}
\includegraphics[width=1.1\linewidth]{figures/Figure 2.png}
\captionsetup{font=normalsize}
\caption*{Figure 2: Histograms of Mortgage Yield across Predictor Variables}
\end{minipage}

\end{figure}
\vspace{-0.5cm}

With deeper analysis, although the variation across SMSAs is small, we
see that regional differences still exist in Mortgage Yields, possibly
due to economic factors like savings, loan terms, and regional banking
practices. The histograms confirm the distribution of the explanatory
variables:

The Loan-to-Mortgage Ratio (**X1**) shows low variance, possibly
indicating limited variability across regions. Distance from Boston
(**X2**) displays a wide and almost homogeneous distribution, reflecting
substantial geographic spread among SMSAs. The right-skewed
distributions of Savings per New Unit Built (**X3**) and Savings per
Capita (**X4**) suggest that a few cities have notably higher savings
levels. Population Increase (**X5**) is also highly right-skewed with
one potential major outlier, indicating that most regions had moderate
growth, while a few experienced rapid expansion. Finally, the percentage
of First Mortgages from Inter-Regional Banks (**X6**) show that most
cities relying minimally on external financing and a few showing heavy
dependence. Overall, the data suggests regional variation in housing
finance conditions, credit accessibility, and Mortgage market dynamics.

## Bivariate analysis

### Graphical analysis

The Association Matrix provides a quick visual assessment of bivariate
relationships (how each variable relates to the others and
\texttt{mortYld}), of types of associations among predictors (if a
relationship looks linear, curved or weak, as well as positive or
negative), and of outlier presence. It complements numerical analyses
like the correlation matrix. We can see that most of the plots are
random dispersion, while some are linear, and some are curved.
\textbf{X3} is positively associated with \textbf{X4} and negatively
with \textbf{X5}. \textbf{X2} and \textbf{X3} are negatively
exponentially associated. On another side, \textbf{X6} is negatively
associated with \textbf{X3}.

\begin{figure}[H]
\centering

\begin{minipage}[t]{0.45\textwidth}
\centering
\includegraphics[width=1\linewidth]{figures/Figure 3.png}
\captionsetup{font=normalsize}
\caption*{Figure 3: Association Matrix Between Mortgage Yield and Predictor Variables}
\end{minipage}

\end{figure}

Let's take a closer look into the Association Matrix, regarding the
relationship between Mortgage Yield (%) and the explanatory variables
(x-axis), representing the first row in the precedent figure.\

As \textbf{X1} increases, the Mortgage Yield increases. This suggests a
positive correlation, and that higher Loan-to-Mortgage Ratios (more
borrowed money relative to the property value) are associated with
higher Mortgage Yields. \textbf{X2} reveals a positive correlation with
\texttt{mortYld}. Boston represents a major financial center with
surplus capital.

Regions further from Boston might have higher Yields. We observe that
**X3** is negatively correlated with `mortYld`. This indicates that
areas with more savings dedicated to new construction have better access
to local financing, resulting in lower Mortgage Yields. **X4**'s
influence is less distinguishable but appears to be a weak negative
correlation or a random dispersion. **X5** shows a positive association
which can be seen as a square-root relationship. High population growth
may imply higher demand for housing, increasing Mortgage Yields due to
heightened competition for available funds. We can observe a potential
outlier at the right side of the plot. **X6**'s variation shows no clear
trend. We can interpret that the reliance on external financing does not
significantly influence Mortgage Yields.\
These observations support the findings of Schaaf (1966) stating that
distance from financial centers, risk factors, and local demand for
savings contribute to Mortgage Yield variations. \vspace{-3pt}

### Numerical analysis

\begin{figure}[H]
\centering
\begin{minipage}{0.39\textwidth}
\includegraphics[width=\linewidth]{figures/Figure 5.png}
\captionsetup{font=normalsize}
\caption*{Figure 5: Correlation Heatmap of Mortgage Yield and Predictor Variables. Red = strong positive, Blue = strong negative, White = no correlation.}
\end{minipage}
\hfill
\begin{minipage}{0.59\textwidth}
\vspace{-1cm}
\small
Now, let's take a look at the correlations between each variable and confirm our previous observations:
\textbf{X3} is strongly positively correlated with \textbf{X4} (0.77) and negatively with \textbf{X2} (-0.64), \textbf{X5} (-0.63), and \textbf{X6} (-0.56).
\textbf{X1} and \textbf{X2} exhibit strong positive correlation with \texttt{mortYld}, while \textbf{X5} shows moderate positive correlation, and \textbf{X3} a strong negative one. \textbf{X6} shows moderate positive correlation with \texttt{mortYld} as well. \textbf{X4} shows only weak correlation with \texttt{mortYld}.


This confirms what we saw earlier in the association matrix.

We can then think about removing one of the highly correlated
predictors, to see if multicollinearity affects the regression model.
However, these correlations only indicate if two variables are linearly
associated. Thus, a low value doesn't necessarily mean that the
variables are not correlated in another way.
\end{minipage}


\end{figure}

# Model Fitting

In this analysis, all predictors are continuous variables and each
observation corresponds to a unique SMSA. Since the dataset contains no
grouping or categorical factors with unequal group sizes, this is a
standard multiple regression model with one observation per row.
Therefore, the design is not factorial and does not involve unbalanced
group structures. As a result, the order of the predictors for the
linear regression model does not influence the coefficient estimates,
F-tests, or model interpretation.

We aim to model the relationship between Mortgage Yield and a set of six
explanatory variables using multiple linear regression.\
The multiple linear regression model is defined mathematically as:
\vspace{-0.2cm} $$
\text{mortYld}_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{4i} + \beta_5 X_{5i} + \beta_6 X_{6i} + \varepsilon_i
$$ where:\
• $\text{mortYld}_i$ is the mortgage yield for the i-th SMSA,\
• $X_{1i}$ to $X_{6i}$ are the explanatory variables,\
• $\beta_0$ is the intercept,\
• $\beta_1$ to $\beta_6$ are the regression coefficients,\
• $\varepsilon_i$ is the error term for observation i.

\vspace{0.2cm}

We assume the classical linear regression assumptions:\
1. Linearity: The relationship between each predictor and the outcome is
linear.\
2. Independence: The errors $\varepsilon_i$ are independent across
observations.\
3. Homoscedasticity: The errors have constant variance:
$\text{Var}(\varepsilon_i) = \sigma^2$.\
4. Normality: The errors follow a normal distribution:
$\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$.\
5. No multicollinearity: The predictors are not perfectly linearly
correlated.

\vspace{0.2cm}

The model is fitted using Ordinary Least Squares (OLS), which minimizes
the sum of squared residuals:\
\vspace{-0.2cm} $$
\min_{\boldsymbol\beta} \sum_{i=1}^{18} \left( \text{mortYld}_i - \beta_0 - \sum_{j=1}^{6} \beta_j X_{ji} \right)^2
$$ \vspace{-0.3cm}

## Null Model vs Full Model Comparison

To test whether the explanatory variables significantly improve the
model fit compared to the intercept-only model, we conduct an ANOVA
comparing the null model and the full (alternative) model. In order to
do so, we test the following hypothesis :

-   **Null Hypothesis (**$H_0$):\
    $$
    \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0
    $$\
    The hypothesis suggests the explanatory variables do not improve the
    model.

-   **Alternative Hypothesis (**$H_1$):\
    $$
    \text{At least one } \beta_j \neq 0 \quad \text{for } j = 1, \ldots, 6
    $$ The hypothesis suggests at least one explanatory variable
    significantly contributes to predicting mortgage yield.

```{r}
null_model <- lm(mortYld ~ 1, data = df)
full_model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)

anova_result <- anova(null_model, full_model)

# Format numeric values: if ≥ 0.095 show 2 decimals, else use scientific notation
anova_df <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col)) {
      sapply(col, function(x) {
        if (is.na(x)) {
          NA
        } else if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, anova_result, names(anova_result))
)

anova_df$`Df` <- as.integer(anova_df$`Df`)
anova_df$`Res.Df` <- as.integer(anova_df$`Res.Df`)

anova_df$Model <- c("mortYld ~ 1", "mortYld ~ X1 + X2 + X3 + X4 + X5 + X6")

# Réorganiser les colonnes
anova_df <- anova_df[, c("Model", "Res.Df", "RSS", "Df", "Sum.of.Sq", "F", "Pr..F.")]
colnames(anova_df) <- c("Model", "Residual_DF", "RSS", "DF", "SS", "F-statistic", "p-value")

# Affichage avec kable
kable(anova_df,
      booktabs = TRUE, digits = 2, format = "latex", escape = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```

\vspace{-0.2em}

\noindent \fontsize{12}{14}\selectfont Table 2: ANOVA Comparison Between
Null and Full Models. Residual degrees of freedom (Residual_DF) and
residual sum of squares (RSS) show the unexplained variance. The degrees
of freedom for the model (DF), F-statistic, and p-value test whether the
full model significantly improves the fit compared to the null model. A
significant p-value (typically \< 0.05) indicates that the additional
predictors in the full model provide a better fit.

\par

\vspace{1cm}

The ANOVA comparison between the null model (intercept-only) and the
full model (including all predictors), reveals that the full model
better explains the Mortgage Yield, as shown by the significant
F-statistic and p-value (p \< 0.001). This indicates that at least one
of the predictors is significantly related to Mortgage Yield.

```{r}
model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)
model_summary <- broom::tidy(model)

model_summary <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col) && !grepl("^df", name)) {
      sapply(col, function(x) {
        if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else if (abs(x) >= 0.0095) {
          sprintf("%.3f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, model_summary, names(model_summary))
)

colnames(model_summary) <- c("Term", "Estimate", "Std.Error", "F-statistic", "p-value")


# Build table grob
table1 <- tableGrob(model_summary, rows = NULL)

# Apply styling
for (i in seq_along(table1$grobs)) {
  if (is.null(table1$grobs[[i]]$gp)) table1$grobs[[i]]$gp <- gpar()
  table1$grobs[[i]]$gp$fill <- "#ffffff"
  table1$grobs[[i]]$gp$fontsize <- 12
}

# Add border around entire table
table1 <- gtable_add_grob(
  table1,
  rectGrob(gp = gpar(lwd = 1, col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table1), r = ncol(table1)
)

# ---- HEADLESS LAYOUT ----
# Create virtual layout without displaying
gt <- grobTree(table1)

# Open PNG device first (without drawing yet)
png("figures/full_model_summary_table.png", width = 10, height = 10, units = "in", res = 300, bg = "white")

# Force layout and compute exact size
grid.newpage()
grid.draw(gt)

# Compute exact size after layout
w <- convertWidth(sum(table1$widths), "in", valueOnly = TRUE)
h <- convertHeight(sum(table1$heights), "in", valueOnly = TRUE)

# Close temporary oversized file
invisible(dev.off())

# Re-open final PNG device with correct size
png("figures/full_model_summary_table.png", width = w + 0.1, height = h + 0.1, units = "in", res = 300, bg = "white")
grid.newpage()
grid.draw(table1)
invisible(dev.off())


```

\begin{minipage}{0.42\textwidth}
\includegraphics[width=1\linewidth]{figures/full_model_summary_table.png}
\vspace{-0.1em}
{\fontsize{12}{14}\selectfont Table 3: Summary of Full Linear Model}
\end{minipage}
\hfill
\begin{minipage}{0.55\textwidth}
The Full model explains $\sim$87\% of the variance in Mortgage Yield, and 80\% after adjusting for the number of predictors, which highlights a strong fit. The Residual Standard Error is low, and the overall model is statistically significant, with a very low p-value ($p < 0.001$). Once again, it means that at least one term contributes significantly to explaining the variation in \texttt{mortYld}.
\end{minipage}

\noindent \vspace{-0.4em}

```{r}
model_stats <- glance(model)[, c("r.squared", "adj.r.squared", "sigma", "statistic", "df", "p.value")]

# Rename columns (LaTeX-ready names)
colnames(model_stats) <- c("$R^2$", "Adjusted\\_$R^{2}$", "Std.Error", "F-statistic", "DF", "p-value")

# Reorder columns
model_stats <- model_stats[, c("Adjusted\\_$R^{2}$", "$R^2$", "Std.Error", "DF", "F-statistic", "p-value")]

# Convert to data.frame
model_stats <- as.data.frame(model_stats)

# Apply rounding
model_stats$`$R^2$` <- round(model_stats$`$R^2$`, 2)
model_stats$`Adjusted\\_$R^{2}$` <- sprintf("%.2f", model_stats$`Adjusted\\_$R^{2}$`)
model_stats$`Std.Error` <- sprintf("%.2f", model_stats$`Std.Error`)
model_stats$`F-statistic` <- round(model_stats$`F-statistic`, 2)
model_stats$`DF` <- as.integer(model_stats$`DF`)
model_stats$`p-value` <- formatC(model_stats$`p-value`, format = "e", digits = 2)

# Display table (optional)
kable(model_stats,
      booktabs = TRUE, format = "latex", escape = FALSE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")
```

\noindent \fontsize{12}{14}\selectfont Table 4: Fit Statistics of Full
Linear Model. R² shows the proportion of variance explained, Adjusted R²
accounts for the number of predictors, and Standard Error measures the
average distance of observed values from the fitted values.

\par

\addtocounter{table}{2}
\vspace{-5pt}

The intercept appears to be strongly significant to fit the model (p \<
0.001). On the other hand, most of the variables do not show
statistically significant individual contributions: only **X1** and
**X3** show weak significance (p $\approx$ 0.05), while the other
variables, **X2**, **X5** and **X6**, do not show significant individual
effects. This suggests that a reduced model may be more appropriate.\
We end up with : $\hat{mortYld}$ = 4.29 + 0.020$\cdot$**X1** +
1.36$\cdot$$10^{-5}$$\cdot$**X2** - 1.58$\cdot$$10^{-3}$$\cdot$**X3** +
2.02$\cdot$$10^{-4}$$\cdot$**X4** + 1.28$\cdot$$10^{-3}$$\cdot$**X5** +
2.36$\cdot$$10^{-4}$$\cdot$**X6**

## Make stepwise regression to select the best model

```{r save_stepwise_tables, echo=FALSE, message=FALSE, warning=FALSE}
step_trace_df <- data.frame(
  Step = c("Start", "Step 1", "Step 2", "Step 3"),
  Model = c("X1 + X2 + X3 + X4 + X5 + X6",
            "X1 + X2 + X3 + X4 + X5",
            "X1 + X3 + X4 + X5",
            "X1 + X3 + X4"),
  RSS = c(0.1098, 0.1099, 0.1109, 0.1159),
  AIC = c(-77.79, -79.77, -81.61, -82.81)
)

# Format numeric columns with 2 decimals
step_trace_df$RSS <- sprintf("%.2f", step_trace_df$RSS)
step_trace_df$AIC <- sprintf("%.2f", step_trace_df$AIC)

table1 <- tableGrob(step_trace_df, rows = NULL)

# Make "Model" column twice as wide as the others
n <- ncol(table1)
widths_vec <- rep(1, n)
widths_vec[2] <- 2.5  # Adjust this if "Model" is not first column
table1$widths <- unit(widths_vec, "null")

for (i in seq_along(table1$grobs)) {
  if (is.null(table1$grobs[[i]]$gp)) table1$grobs[[i]]$gp <- gpar()
  table1$grobs[[i]]$gp$fontsize <- 11
  table1$grobs[[i]]$gp$fill <- "#ffffff"
}

table1 <- gtable_add_grob(
  table1,
  rectGrob(gp = gpar(lwd = 1, col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table1), r = ncol(table1)
)


# 2. Stepwise Model Coefficients Table
step_model <- stepAIC(model, direction = "both", trace = FALSE)
step_summary <- broom::tidy(step_model)

step_summary <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col) && !grepl("^df", name)) {
      sapply(col, function(x) {
        if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else if (abs(x) >= 0.0095) {
          sprintf("%.3f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, step_summary, names(step_summary))
)

colnames(step_summary) <- c("Term", "Estimate", "Std.Error", "F-statistic", "p-value")

table2 <- tableGrob(step_summary, rows = NULL)
table2$widths <- unit(rep(1, ncol(table2)), "null")

for (i in seq_along(table2$grobs)) {
  if (is.null(table2$grobs[[i]]$gp)) table2$grobs[[i]]$gp <- gpar()
  table2$grobs[[i]]$gp$fontsize <- 12
  table2$grobs[[i]]$gp$fill <- "#ffffff"
}

table2 <- gtable_add_grob(
  table2,
  rectGrob(gp = gpar(lwd = 1, col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table2), r = ncol(table2)
)

# Save as PNGs
ggsave("figures/stepwise_aic_table.png", table1, width = 4.5, height = 2, dpi = 300)
ggsave("figures/stepwise_coef_table.png", table2, width = 4.53, height = 2, dpi = 300)
```

\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{figures/stepwise_aic_table.png}
\vspace{-1.3em}
\parbox{\linewidth}{\fontsize{12}{14}\selectfont Table 5: Stepwise AIC Process. AIC = Akaike Information Criterion. Lower AIC indicates a better trade-off between model fit and complexity.}
\end{minipage}
\hfill
\raisebox{1.6em}{%
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\linewidth]{figures/stepwise_coef_table.png}
\vspace{-1.3em}
\parbox{\linewidth}{\fontsize{12}{14}\selectfont Table 6: Summary of Final Stepwise Model}
\end{minipage}
}
\addtocounter{table}{2}
\vspace{0.3em}

```{r}
# 3. Fit Statistics of Stepwise Model

# Extraire les stats globales
step_glance <- glance(step_model)[, c("r.squared", "adj.r.squared", "sigma", "statistic", "df", "p.value")]
colnames(step_glance) <- c("$R^2$", "Adjusted\\_$R^{2}$", "Std.Error", "F-statistic", "DF", "p-value")

step_glance <- step_glance[, c("Adjusted\\_$R^{2}$", "$R^2$", "Std.Error", "DF", "F-statistic", "p-value")]


# Convertir en data.frame pour mise en forme fine
step_glance <- as.data.frame(step_glance)

step_glance$`$R^2$` <- round(step_glance$`$R^2$`, 2)
step_glance$`Adjusted\\_$R^{2}$` <- sprintf("%.2f", step_glance$`Adjusted\\_$R^{2}$`)
step_glance$`Std.Error` <- sprintf("%.3f", step_glance$`Std.Error`)
step_glance$`F-statistic` <- round(step_glance$`F-statistic`, 2)
step_glance$`DF` <- as.integer(step_glance$`DF`)
step_glance$`p-value` <- formatC(step_glance$`p-value`, format = "e", digits = 2)

# Afficher en tableau LaTeX
kable(step_glance,
      booktabs = TRUE, format = "latex", escape = FALSE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```

\begin{center}
\vspace{-1.5em}
{\fontsize{12}{14}\selectfont Table 7: Fit Statistics of Stepwise Model\par}
\end{center}

The Stepwise regression process identifies **X1**, **X3**, and **X4** as
the most significant predictors of Mortgage Yield, constituting the
final model.

It is interesting to note that **X4** appears among the 3 most
significant predictors although it shows very weak correlation in the
Correlation Matrix. Multiple regression measures the effect of each
variable while holding all others constant. As **X4** has very strong
correlation with **X3** (0.77), holding **X3** can make the unique
contribution of **X4** clearer.

The final Stepwise model explains approximately 83.4% of the variance in
Mortgage Yield using only these three predictors. The AIC doesn't
increases a lot when keeping more predictors, meaning that even if these
predictors can still be statistically valid to keep, they are not so
useful to the model. Though the final model is simpler, it explains the
data just as well or better than more complex models. The Residual
Standard Error (0.09) is low, and the overall model is highly
significant (p \< 0.001), indicating a good fit.\
We end up with : $\hat{mortYld}$ = 4.22 + 0.022$\cdot$**X1** -
1.86$\cdot$$10^{-3}$$\cdot$**X3** + 2.25$\cdot$$10^{-4}$$\cdot$**X4**\
\
Let's now try a model with 2-way interactions.

```{r}
# 1. Coefficients Table
interaction_model <- lm(mortYld ~ X1 + X3 + X4 + X1:X3 + X1:X4 + X3:X4, data = df)
interaction_coef <- broom::tidy(interaction_model)

interaction_coef <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col) && !grepl("^df", name)) {
      sapply(col, function(x) {
        if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else if (abs(x) >= 0.0095) {
          sprintf("%.3f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, interaction_coef, names(interaction_coef))
)

colnames(interaction_coef) <- c("Term", "Estimate", "Std.Error", "F-statistic", "p-value")

# Génération du tableau
table1 <- gridExtra::tableGrob(interaction_coef, rows = NULL)

# Appliquer fond blanc et taille police
for (i in seq_along(table1$grobs)) {
  if (is.null(table1$grobs[[i]]$gp)) table1$grobs[[i]]$gp <- gpar()
  table1$grobs[[i]]$gp$fill <- "#ffffff"
  table1$grobs[[i]]$gp$fontsize <- 12
}

# Cadre noir autour
table1 <- gtable::gtable_add_grob(
  table1,
  grobs = grid::rectGrob(gp = gpar(col = "black", fill = NA)),
  t = 1, l = 1, b = nrow(table1), r = ncol(table1)
)

w <- convertWidth(sum(table1$widths), "in", valueOnly = TRUE)
h <- convertHeight(sum(table1$heights), "in", valueOnly = TRUE)

w <- w + 0.01
h <- h + 0.01

# Export PNG ajusté au contenu
png("figures/interaction_model_coef.png", width = w, height = h, units = "in", res = 300, bg = "white")
grid.draw(table1)

invisible(dev.off())


```

\begin{minipage}{0.5\textwidth}
\includegraphics[width=1.25\linewidth]{figures/interaction_model_coef.png}
\vspace{-0.3em}
\fontsize{12}{12}\selectfont Table 8: Summary of 2-way Interaction Model
\end{minipage}
\hfill
\begin{minipage}{0.35\textwidth}
The 2-way Interaction model, which is more complex than the Stepwise
model, explains approximately 79.9\% of the variance in Mortgage Yield.
The Residual Standard Error (0.10) is low, and the overall model is
highly significant (p < 0.001), indicating that at least one of the
terms has a significant influence on Mortgage Yield.
\end{minipage}

```{r}
interaction_stats <- glance(interaction_model)[, c("r.squared", "adj.r.squared", "sigma", "statistic", "df", "p.value")]

# Rename columns (LaTeX-ready names)
colnames(interaction_stats) <- c("$R^2$", "Adjusted\\_$R^{2}$", "Std.Error", "F-statistic", "DF", "p-value")

# Reorder columns
interaction_stats <- interaction_stats[, c("Adjusted\\_$R^{2}$", "$R^2$", "Std.Error", "DF", "F-statistic", "p-value")]

# Convert to data.frame
interaction_stats <- as.data.frame(interaction_stats)

interaction_stats$`$R^2$` <- round(interaction_stats$`$R^2$`, 2)
interaction_stats$`Adjusted\\_$R^{2}$` <- sprintf("%.2f", interaction_stats$`Adjusted\\_$R^{2}$`)
interaction_stats$`Std.Error` <- sprintf("%.2f", interaction_stats$`Std.Error`)
interaction_stats$`F-statistic` <- round(interaction_stats$`F-statistic`, 2)
interaction_stats$`DF` <- as.integer(interaction_stats$`DF`)
interaction_stats$`p-value` <- formatC(interaction_stats$`p-value`, format = "e", digits = 2)


# Display table (optional)
kable(interaction_stats,
      booktabs = TRUE, format = "latex", escape = FALSE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```

\begin{center}
\vspace{-1.5em}
{\fontsize{12}{14}\selectfont Table 9: Fit Statistics of 2-way Interaction Model\par}
\end{center}

\addtocounter{table}{2}

None of the variables show statistically significant individual
contributions: only the intercept appears to be moderately significant
to fit the model (p \< 0.05). This suggests that a reduced model may be
more appropriate.\
We end up with : $\hat{mortYld}$ = 5.37 +
6.91$\cdot$$10^{-3}$$\cdot$**X1** - 1.04$\cdot$$10^{-4}$$\cdot$**X3** -
9.10$\cdot$$10^{-4}$$\cdot$**X4** -
2.09$\cdot$$10^{-5}$$\cdot$**X1**:**X3** +
1.50$\cdot$$10^{-5}$$\cdot$**X1**:**X4** -
4.75$\cdot$$10^{-8}$$\cdot$**X3**:**X4**\
\
We decided not to include a 3-way Interaction model in our analysis.
Given the small sample size (18 observations), adding high-order
interactions would significantly reduce degrees of freedom and increase
the risk of overfitting. Moreover, 3-way interactions are often
difficult to interpret meaningfully.

## Model Comparison

```{r, fig.width=10, fig.height=2}
# Create comparison table
# Create the data.frame with simple names first
model_metrics <- data.frame(
  Model = c("Full Model", "Stepwise Model", "2-Way Interaction Model"),
  Adjusted_R2 = c(
    summary(full_model)$adj.r.squared,
    summary(step_model)$adj.r.squared,
    summary(interaction_model)$adj.r.squared
  ),
  R2 = c(
    summary(full_model)$r.squared,
    summary(step_model)$r.squared,
    summary(interaction_model)$r.squared
  ),
  RSE = c(
    summary(full_model)$sigma,
    summary(step_model)$sigma,
    summary(interaction_model)$sigma
  ),
  AIC = c(
    AIC(full_model),
    AIC(step_model),
    AIC(interaction_model)
  ),
  `F-statistic` = c(
    summary(full_model)$fstatistic[1],
    summary(step_model)$fstatistic[1],
    summary(interaction_model)$fstatistic[1]
  )
)

# Then rename the columns for LaTeX display
colnames(model_metrics) <- c("Model", "Adjusted_R²", "R²", "RSE", "AIC", "F-statistic")

model_metrics$RSE <- c(
  sprintf("%.2f", model_metrics$RSE[1]),
  sprintf("%.3f", model_metrics$RSE[2]),
  sprintf("%.2f", model_metrics$RSE[3])
)


# Display table
kable(model_metrics, digits = 2, escape=FALSE) %>%
  kable_styling(font_size = 8, full_width = FALSE, position = "center")

```

\begin{center}
\vspace{-0.5em}
{\fontsize{12}{14}\selectfont Table 10: Comparison of Model Performance Metrics. RSE (residual standard error) reflects the typical size of prediction errors; lower values indicate better fit.\par}
\end{center}

The Stepwise model offers the best trade-off between simplicity and
performance: it has the lowest AIC (\~29.7), demonstrating the best
model fit among the three. Despite having a slightly lower R² than the
Full and 2-ways Interactions model, it achieves the highest Adjusted R².
It also has the lowest Residual Standard Error (0.09) and the highest
F-statistic (\~29.5). This confirms the overall model significance and
parsimony.

```{r}
# Compare with ANOVA and convert to data frame
anova_result <- anova(step_model, interaction_model)

# Format numeric values: if ≥ 0.095 show 2 decimals, else use scientific notation
anova_df <- as.data.frame(
  Map(function(col, name) {
    if (is.numeric(col)) {
      sapply(col, function(x) {
        if (is.na(x)) {
          NA
        } else if (abs(x) >= 0.095) {
          sprintf("%.2f", x)
        } else {
          formatC(x, format = "e", digits = 2)
        }
      })
    } else {
      col
    }
  }, anova_result, names(anova_result))
)

anova_df$`Df` <- as.integer(anova_df$`Df`)
anova_df$`Res.Df` <- as.integer(anova_df$`Res.Df`)
anova_df$Model <- c("Stepwise model", "Interaction model")

# Réorganiser les colonnes
anova_df <- anova_df[, c("Model", "Res.Df", "RSS", "Df", "Sum.of.Sq", "F", "Pr..F.")]
colnames(anova_df) <- c("Model", "Residual_DF", "RSS", "DF", "SS", "F-statistic", "p-value")

# Affichage avec kable
kable(anova_df,
      booktabs = TRUE, digits = 2, format = "latex", escape = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position", position = "center")

```

\begin{center}
\vspace{-1.6em}
{\fontsize{12}{14}\selectfont Table 11: ANOVA Comparison Between Stepwise and Interaction Models\par}
\end{center}

An ANOVA is then conducted to compare the Stepwise Model and the
Interaction Model, which are nested — the Interaction Model extends the
Stepwise Model by including additional two-way interaction terms. The
test yields an F-statistic of 0.18 and a p-value of 0.91, indicating
that the additional interaction terms do not significantly reduce the
residual variance. As a result, the simpler model with only main effects
(**X1**, **X3**, and **X4**) truly provides the best fit, as it also
offers comparable explanatory power and better interpretability.

# Model assumptions and Diagnostics

In order to trust the results of our regression model, we must ensure
that the residuals satisfy the following assumptions:

1.  The residuals have an expected value (mean) of 0:
    $\mathbb{E}[\varepsilon_i] = 0$
2.  The residuals are homoscedastic (have constant variance):
    $\text{Var}(\varepsilon_i) = \sigma^2$
3.  The residuals are uncorrelated:
    $\text{Cov}(\varepsilon_i, \varepsilon_j) = 0$ for $i \neq j$
4.  The residuals are normally distributed:
    $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$

We evaluate these assumptions using residual diagnostic plots.

## Independence evaluation

```{r, fig.width=7.5, fig.height=2.5}
par(mfrow = c(1, 3),        
    mar = c(4, 4, 1, 1),       # bottom margin increased to fit caption
    mgp = c(2, 0.6, 0),     
    cex.main = 1.2,     # Title size (not used now)
    cex.lab = 1.2,      
    cex.axis = 1.0)

# Residuals vs. Fitted Values Plot (no top title)
plot(fitted(model), resid(model),
     main = "",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 19, col = "blue")
res_fit_lm <- lm(resid(model) ~ fitted(model))
abline(res_fit_lm, col = "red", lwd = 2)
coef_res_fit <- coef(res_fit_lm)
text(min(fitted(model)), max(resid(model)) * 0.8, 
     labels = paste0("y = ", formatC(coef_res_fit[2], format = "e", digits=2), "\u00B7x\n+ ", formatC(coef_res_fit[1], format = "e", digits=2)), 
     pos = 4, col = "red", cex = 1.2)

# Residuals vs. Observation Order Plot (no top title)
plot(resid(model), type = "p",
     main = "",
     xlab = "Observation Index", ylab = "Residuals",
     pch = 19, col = "darkgreen")
res_obs_lm <- lm(resid(model) ~ seq_along(resid(model)))
abline(res_obs_lm, col = "red", lwd = 2)
coef_res_obs <- coef(res_obs_lm)
text(1, max(resid(model)) * 0.9, 
     labels = paste0("y = ", formatC(coef_res_obs[2], format = "e", digits=2), "\u00B7x + ", sprintf("%.3f", coef_res_obs[1])), 
     pos = 4, col = "red", cex = 1.2)

# Scale-Location Plot (no top title)
plot(fitted(model), sqrt(abs(resid(model))),
     main = "",
     xlab = "Fitted Values", ylab = expression(sqrt(abs(Residuals))),
     pch = 19, col = "purple")
scale_loc_lm <- lm(sqrt(abs(resid(model))) ~ fitted(model))
abline(scale_loc_lm, col = "red", lwd = 2)
coef_scale_loc <- coef(scale_loc_lm)
text(min(fitted(model)), max(sqrt(abs(resid(model)))) * 0.95,
     labels = paste0("y = ", round(coef_scale_loc[2], 2), "\u00B7x + ", round(coef_scale_loc[1], 2)), 
     pos = 4, col = "red", cex = 1.2)


# Reset plotting parameters
par(mfrow = c(1, 1))


```

\begin{figure}[htbp]
\vspace{-1em}
  \centering
  \begin{minipage}[b]{0.3\linewidth}
    {\fontsize{12}{14}\selectfont Figure 6: Residuals vs Fitted Plot. It displays residual spread to assess homoscedasticity; no clear pattern indicates constant variance.\par}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.3\linewidth}
    {\fontsize{12}{14}\selectfont Figure 7: Residuals vs Observation Order Plot. It shows residuals over SMSAs to detect trends or autocorrelation; randomness suggests independence.\par}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.3\linewidth}
    {\fontsize{12}{14}\selectfont Figure 8: Scale-Location Plot. It shows the variance of residuals versus fitted values to check for homoscedasticity; a horizontal pattern suggests constant variance.\par}
  \end{minipage}
\end{figure}

The `Residuals VS Fitted Values` plot on Figure 6 checks for linearity and constant variance. Ideally, residuals should be symmetrically scattered around zero with no clear pattern or funnel shape. In our case, the residuals are randomly dispersed with no visible trend, and the red regression line is nearly flat (slope = \(-1.05 \times 10^{-16}\)), suggesting that the residuals have approximately zero mean and constant variance. Therefore, assumptions 1 and 2 are satisfied.

The `Residuals VS Observation Order` on Figure 7 is used to detect autocorrelation in the residuals (assumption 3). A patternless distribution across observations indicates independence. The red regression line has a slope of \(-0.0042\), which is close to zero, and residuals appear randomly scattered, suggesting that residuals are **not autocorrelated**. Hence, assumption 3 appears is verified.

Lastly, the `Scale-Location` plot on Figure 8, shows the square root of standardized residuals vs fitted values. A line with constant spread indicates constant variance : although the slope is somewhat negative (\(-0.29\)), the spread remains relatively even. There is no clear increasing or decreasing funnel shape. This is a sign that our model doesn't suffer from heteroscedasticity and is likely a good fit : this supports the homoscedasticity assumption.

In conclusion, based on Figures 6–8, we find that the residuals have a mean close to zero, appear homoscedastic, and show no sign of autocorrelation. Therefore, assumptions 1, 2, and 3 are reasonably satisfied.


```{r save_vif_table, echo=FALSE, message=FALSE, warning=FALSE}
# Compute VIF
vif_values <- vif(step_model)
vif_df <- data.frame(
  Variable = rownames(as.data.frame(vif_values)),
  VIF = round(as.numeric(vif_values), 2)
)

# Create grob table with white background and black borders
vif_table <- tableGrob(vif_df, rows = NULL)

# Equalize column widths
vif_table$widths <- unit(rep(1, ncol(vif_table)), "null")

# White background + font size
for (i in seq_along(vif_table$grobs)) {
  if (is.null(vif_table$grobs[[i]]$gp)) vif_table$grobs[[i]]$gp <- gpar()
  vif_table$grobs[[i]]$gp$fill <- "#ffffff"
  vif_table$grobs[[i]]$gp$fontsize <- 12
}

# Add black border
vif_table <- gtable_add_grob(
  vif_table,
  rectGrob(gp = gpar(col = "black", fill = NA, lwd = 1)),
  t = 1, l = 1, b = nrow(vif_table), r = ncol(vif_table)
)

# Save as PNG
ggsave("figures/vif_table.png", vif_table, width = 1.5, height = 1.3, dpi = 300)

```

\vspace{-0.5em}

## Multicolinearity diagnostic

\noindent

\begin{minipage}{0.6\textwidth}
\vspace{0.3cm}
We assess multicollinearity using the Variance Inflation Factor (VIF). All variables in the final model have VIF values below 5 (see Table 12), indicating that none of the predictors are highly correlated with each other.

Even though variables \(X_3\) and \(X_4\) had a pairwise correlation of 0.77, the VIF values of 4.55 and 3.35 respectively suggest acceptable collinearity. Therefore, these variables still provide enough unique, non-redundant information to justify keeping them in the model
\vspace{0.2cm}
In conclusion, there is no evidence of problematic multicollinearity, and all explanatory variables contribute distinct information to the model.

\end{minipage}
\hfill
\begin{minipage}{0.37\textwidth}
\vspace{-3em}
  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/vif_table.png}
    \vspace{-0.5em}
    \captionsetup{font=normalsize}
    \caption*{Table 12: Variance Inflation Factors (VIF). Values > 5: potential multicollinearity. Values > 10: strong multicollinearity.}
  \end{figure}
\end{minipage}

\vspace{-1em}

## Normality Check

```{r save_qqplot, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}

png("figures/qqplot_residuals.png", width = 650, height = 650, res = 150)

par(pty = "s", 
    mar = c(4, 3.5, 2, 1),
    mgp = c(2, 0.5, 0),
    cex.main = 1.6,
    cex.lab = 1.6,
    cex.axis = 1.4)

qqnorm(resid(step_model), 
       main = "", 
       xlab = "Theoretical Quantiles", 
       ylab = "Sample Quantiles",
       pch = 19, col = "steelblue",
       xlim = c(-2, 2), ylim = c(-0.2, 0.2))
qqline(resid(step_model), col = "red")

dev.off()

```

\noindent

\begin{minipage}{0.65\textwidth}
\justifying
The \texttt{Q-Q plot of Residuals} on Figure 9 checks whether the residuals follow a normal distribution (assumption 4). If residuals are normally distributed, the points should align closely with the 45-degree reference line. 
In our plot, most points fall near the line, particularly in the center, suggesting that the central portion of the distribution follows a normal pattern. However, several points at both tails deviate from the line (at the lower and upper ends of the theoretical quantiles), indicating potential departures from normality in the extremes : this could reflect outliers or heavy-tailed behavior.
With only 18 observations, such deviations can be expected and are not strong evidence against normality.

\vspace{0.3cm}
In conclusion, the residuals appear to be approximately normally distributed, and assumption 4 is reasonably met.
\end{minipage}
\hfill
\begin{minipage}{0.33\textwidth}
\centering
\vspace{-2em}  % Reduce space before image
\includegraphics[width=0.9\linewidth]{figures/qqplot_residuals.png}
\vspace{-1em}
\parbox{\linewidth}{\fontsize{12}{14}\selectfont Figure 9: Q-Q Plot of Residuals. Deviations from the line suggest non-normality of model errors.}

\end{minipage}

# Conclusion

The final estimated model is : $\hat{mortYld}$ = 4.22 +
0.022$\cdot$**X1** - 1.86$\cdot$$10^{-3}$$\cdot$**X3** +
2.25$\cdot$$10^{-4}$$\cdot$**X4**\
where **X1** is the Loan-to-Mortgage Ratio, **X3** is the Savings per
New Unit Built, and **X4** is the Savings per Capita.

The analysis shows that these variables significantly impact Mortgage
Yield. Mortgage Yield is positively influenced by the Loan-to-Mortgage
Ratio, indicating that higher loan amounts relative to mortgages may
lead to better returns for lenders. Conversely, Mortgage Yield is
negatively impacted by Savings per New Unit Built, suggesting that more
capital saved for construction could reduce reliance on mortgages,
leading to lower returns. Finally, Savings per Capita has a positive,
though small, effect on Mortgage Yield. As individual savings increase,
it may signal a more financially stable environment, leading to slightly
better mortgage performance.

While the assumptions of linear regression are generally satisfied,
there are some minor deviations. The model shows strong predictive
performance, accounting for 83.4% of the variance in Mortgage Yield,
with homoscedasticity nearly achieved.

Future improvements could include exploring additional predictors,
testing for non-linear relationships, or refining the model to better
capture any residual heteroscedasticity.

# References
