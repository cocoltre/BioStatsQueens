---
title: "Predicting Mortgage Yield using Regression Analysis"
author: "Group 42"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    toc: false
    latex_engine: pdflatex
    keep_tex: true
geometry: margin=2.5cm
fontsize: 11pt
header-includes:
  - \usepackage{graphicx}
  - \usepackage{amsmath}
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{fancyhdr}
  - \usepackage{ragged2e}
  - \usepackage{multicol}
  - \justifying
  - \pagestyle{fancy}
<<<<<<< HEAD
  - \fancyhead[L]{Delandre, Garcia, Biocchi, Leteurte}
=======
  - \fancyhead[L]{Clara Delandre, Majandra Garcia, Paola Biocchi, Coline Leteurtre}
>>>>>>> 8f6a53ecbe505010c0a1093b8a29576b20010806
  - \fancyfoot[C]{\thepage}
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction

The study of A. H. Schaaf, 1966, "Regional Differences in Mortgage
Financing Costs" investigates the existence and causes of regional
differences in mortgage financing costs in the United States. While these
differences in mortgage yields were decreasing in the early 20th century, they suprisingly remained stable after World War II. The paper explores two main explanations for this phenomenon:\
\
**1.** Differences in investment value due to risk, terms, and
liquidity.\
**2.** Market imperfections such as legal barriers and information
gaps.\
\
The data used in this study comes from the Federal Home Loan Bank Board,
which contains interest rates and fees in 18 SMSAs (Standard
Metropolitan Statistical Areas). The findings suggest that distance from
major financial centers, risk levels, and local demand for savings
significantly affect mortgage yields. However, market structure and
overall savings levels play a lesser important role.\
\
The aim of this report is to analyze the data and develop a predictive
model to predict Mortgage Yield (`mortYld` in %) based on 6 explanatory
variables:\
\
- **X1:** Loan-to-Mortgage Ratio, in % → High values indicate low down
payments.\
- **X2:** Distance from Boston, in miles → Measures regional proximity
to financial centers.\
- **X3:** Savings per New Unit Built, in \$ → Indicator of regional
credit demand.\
- **X4:** Savings per Capita, in \$ → Measures local savings levels
(credit supply).\
- **X5:** Population Increase, 1950-1960, in % → Proxy for housing
demand growth.\
- **X6:** Percentage of first mortgages from inter-regional banks, in %
→ Indicator of external financing reliance.

------------------------------------------------------------------------

# Exploratory Data Analysis (EDA)

## Load Data and Libraries

```{r global-options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```

```{r}
# Load necessary libraries
library(ggplot2)
library(tidyverse)
library(dplyr)
library(knitr)
library(kableExtra)
library(xtable)
library(MASS)
library(car)
library(magrittr)
library(patchwork)
library(gridExtra)
library(grid)
library(reshape2)

# Read data
df <- read.csv("myield.csv")

# Rename columns for easier reference
colnames(df) <- c("smsa", "mortYld", "X1", "X2", "X3", "X4", "X5", "X6")

# Display first few rows
kable(head(df), caption = "First few rows of the dataset") %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

```{r, include=FALSE}
# Count missing values
colSums(is.na(df))

```
Here is a display of the first few rows of the dataset. Each SMSA is described by
its Mortgage Yield (`mortYld`) as the dependent variable and six explanatory
variables (X1 to X6). All data consist of numerical values and quick checking confirms that there are no missing values in any region.

## Univariate Analysis

### Summary Statistics

```{r}
# Summary Statistics
summary_stats <- summary(df[, colnames(df) != "smsa"])
kable(summary_stats, caption = "Summary Statistics of Variables") %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

Through this summary, we already observe that mortgage yields don’t vary much across regions. Most values are between 5.2% and
6.2%, suggesting relatively stable mortgage rates.

Loan-to-Mortgage Ratios (X1) are concentrated in between 67% and 78.1%, indicating relatively consistent lending practices across regions. Distance from Boston (X2) has a vast range (0–3162 miles), highlighting geographical diversity and potential financial access disparities. Savings per New Unit Built (X3) and Savings per Capita (X4) are characterized by means bigger than medians, representing right-skewed distributions, thus suggesting regional imbalances in credit demand and supply/in housing affordability across regions. Population Increase (X5) from 1950 to 1960 varies widely (7.5–88.9%), reflecting differing housing market pressures. Lastly, Percentage of First Mortgages from Inter-Regional Banks (X6) spans from 2.0% to 51.3%, meaning that some areas depend heavily on external financing while others rely more on local institutions.


### Graphical Representation

```{r, fig.width=13, fig.height=5}

# Sort data by decreasing mortgage yield
df <- df %>% arrange(desc(mortYld))

# Define bar chart (corrected and stored as bar_plot)
bar_plot <- ggplot(df, aes(x = mortYld, y = reorder(smsa, mortYld))) + 
  geom_bar(stat = "identity", fill = "blue") +
  coord_cartesian(xlim = c(4.5, 6.5)) +
  labs(title = "Mortgage Yield by SMSA", x = "Mortgage Yield (%)", y = "SMSA (City)") +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.y = element_text(size = 9),
    plot.title = element_text(hjust = 0.5)
  )

# Define variable names and colors
var_names <- c("Loan-to-Mortgage Ratio", "Distance from Boston", 
               "Savings per New Unit Built", "Savings per Capita", 
               "Population Increase", "First Mortgage from Inter-Regional Banks")
hist_colors <- c("red", "blue", "green", "purple", "orange", "cyan")

# Create histogram plots
hist_plots <- lapply(1:6, function(i) {
  ggplot(df, aes_string(x = paste0("X", i))) +
    geom_histogram(fill = hist_colors[i], color = "black", bins = 8) +
    labs(title = var_names[i], x = NULL, y = "Frequency") +
    theme_minimal(base_size = 10)
})

# Combine histograms in 2x3 grid
hist_grid <- (hist_plots[[1]] | hist_plots[[2]] | hist_plots[[3]]) /
             (hist_plots[[4]] | hist_plots[[5]] | hist_plots[[6]])

# Combine bar chart and histograms side-by-side
final_plot <- bar_plot + hist_grid + plot_layout(widths = c(1.3, 2))

# Print final plot
final_plot
```

With deeper analysis, although the variation across SMSAs is small, we see that regional differences still exist in mortgage yields, possibly due to economic factors like savings, loan terms, and regional banking practices.

The histograms confirm the distribution of the explanatory variables.

The Loan-to-Mortgage Ratio (X1) shows low variance with most values
concentrated between 67% and 80%, possibly indicating limited
variability across regions.
Distance from Boston (X2) displays a wide and almost homogeneous distribution, reflecting
substantial geographic spread among SMSAs.
Savings per New Unit Built (X3) and Savings per Capita (X4) both exhibit
right-skewed distributions, suggesting that a few cities have notably
higher savings levels.
Population Increase (X5) is also highly right-skewed with one major outlier (increase of %~%25%),
indicating that most regions had moderate growth, while a few experienced
rapid expansion.
Finally, the percentage of First Mortgages from Inter-Regional Banks
(X6) is also right-skewed, with most cities relying minimally on
external financing and a few showing heavy dependence.

Overall, the data suggests regional variation in housing finance conditions, credit accessibility, and mortgage market dynamics.

## Bivariate Numerical Analysis

### Association Analysis

```{r, fig.width=4.5, fig.height=4.5}
# Réinitialisation des paramètres graphiques
par(mfrow = c(1, 1))
par(mai = c(0.3, 0.3, 0.5, 0.3))  # marges réduites : bas, gauche, haut, droite
par(oma = c(0, 0, 0, 0))          # aucune marge extérieure

# Sélection des variables numériques
selected_vars <- df[, c("mortYld", "X1", "X2", "X3", "X4", "X5", "X6")]

# Affichage du scatterplot matrix
pairs(selected_vars,
      main = "Association Matrix of Variables and mortYld",
      cex.main = 0.9,       # taille du titre réduite
      col = "blue",
      pch = 19)

```

The Association Matrix provides a quick visual assessment of linearity,
strength of linear associations among predictors, and outlier detection. It
complements numerical analyses like the correlation matrix and VIF.

We visualize bivariate relationships, i.e. how each variable relates to
the others and mortYld, and assess if a relationship looks
linear, curved or weak, as well as positive or negative. We can also spot outliers
or cities that don’t follow the general trend.

We can see that most of the plots are random dispersion, while some are
linear, and some are curved. X3 seems to be positively associated with
X4 and negatively with X5. X2 and X3 seem negatively exponentially associated.
X6 seems to be negatively associated with X3.

Let's take a closer look into the Association Matrix, regarding the
relationship between Mortgage Yield (%) and the explanatory variables
(x-axis), representing the first row in the precedent figure.

```{r, fig.width=10, fig.height=5}
# Set up a 2x3 grid for multiple scatter plots
par(mfrow = c(2, 3))

# Define variable names
var_names <- c("Loan-to-Mortgage Ratio", "Distance from Boston", 
               "Savings per New Unit Built", "Savings per Capita", 
               "Population Increase", "First Mortgage from Inter-Regional Banks")

# Define colors for better visualization
point_colors <- c("red", "blue", "green", "purple", "orange", "cyan")

# Loop through each variable (X1 to X6) and create a scatter plot with mortYld on the Y-axis
for (i in 1:6) {
  plot(df[[paste0("X", i)]], df$mortYld,
       main = paste(var_names[i], "vs Mortgage Yield"),
       xlab = var_names[i], ylab = "Mortgage Yield (%)",
       col = point_colors[i], pch = 19)
}

# Reset plotting parameters to default
par(mfrow = c(1, 1))

```

**1. Loan-to-Mortgage Ratio:** As this ratio increases, the Mortgage
Yield increases. This suggests a positive correlation, and that higher
Loan-to-Mortgage Ratios (more borrowed money relative to the property
value) are associated with higher mortgage yields.\
**2. Distance from Boston:** There is a positive correlation. Boston
represents a major financial center with surplus capital. Regions
further from Boston might have higher yields.\
**3. Savings per New Unit Built :** There seems to be a negative correlation.
This indicates that areas with more savings dedicated to new
construction have better access to local financing, resulting in lower
mortgage yields.\
**4. Savings per Capita:** The relationship is less distinguishable but appears to
be a weak negative correlation or a random dispersion.\
**5. Population Increase:** There is a positive association which can be
seen as a square-root relationship. High population growth may imply
higher demand for housing, increasing mortgage yields due to heightened
competition for available funds. We can observe a potential outlier at
the right side of the plot.\
**6. First Mortgage from Inter-Regional Banks:** No clear trend. It
seems like the reliance on external financing (measured by the
Percentage of First Mortgages from Inter-Regional Banks) does not
significantly influence mortgage yields.\
\
**To resume:**\
- X1, X2 and X5 seem to be the most influential variables positively
correlated with Mortgage Yield.\
- X3 is the most influential variable negatively
correlated with Mortgage Yield.\
- X6 shows moderate positive influence on Mortgage Yield.
- X4 variable shows a weak relationship with mortgage yields.\
These observations support the findings of Schaaf (1966) that distance
from financial centers, risk factors, and local demand for savings
contribute to yield variations.\

### Correlation analysis

```{r, fig.width=5, fig.height=5}
library(reshape2)

# Compute correlation matrix and reshape
cor_matrix <- cor(df[, c("mortYld", "X1", "X2", "X3", "X4", "X5", "X6")])
melted_cor <- melt(cor_matrix)

# Plot
ggplot(data = melted_cor, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile(color = "white") +
  geom_text(aes(label = round(value, 2)), size = 3) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name = "value") +
  coord_fixed() + 
  theme_minimal(base_size = 8) +
  theme(
    plot.title = element_text(size = 10, face = "bold", hjust = 0.5, margin = margin(b = 5)),
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.margin = margin(5, 5, 5, 5)
  ) +
  labs(title = "Correlation Heatmap")

```
Now, let's take a look at the correlations between each variable and confirm our previous observations.

X3 is are strongly positively correlated to X4 (0.77) and moderately negatively correlated to X2 (-0.64), X5 (-0.63) and X6 (-0.56). This confirms our observations with the Association Matrix.

X1 and X2 exhibit strong positive correlation with mortYld, while X5 shows moderate positive correlation, and X3 a
strong negative one. X6 shows moderate positive correlation with mortYld as well. X4 shows only weak correlation with mortYld. In consequence, we can confirm our previous statements about the scatter-plots.

We can then think about removing one of the highly correlated
predictors, to see if multicollinearity affects the regression model. However, these correlations only indicate if two variables are linearly associated. A low value doesn't necessarily mean that the variables are not correlated in another way.


------------------------------------------------------------------------

# Model Fitting

In this analysis, all predictors are continuous variables and each
observation corresponds to a unique SMSA. Since the dataset contains no
grouping or categorical factors with unequal group sizes, this is a
standard multiple regression model with one observation per row.
Therefore, the design is not factorial and does not involve unbalanced
group structures. As a result, the order in which predictors are entered
into the lm() function does not influence the coefficient estimates,
F-tests, or model interpretation.

## Pairwise Simple Regressions

```{r, fig.width=10, fig.height=2}
# Pairwise Simple Regressions
models_simple <- lapply(1:6, function(i) {
  formula <- as.formula(paste("mortYld ~ X", i, sep = ""))
  lm(formula, data = df)
})

simple_summaries <- lapply(models_simple, summary)

data.frame(
  Predictor = paste0("X", 1:6),
  R_squared = sapply(simple_summaries, function(m) round(m$r.squared, 3)),
  p_value = sapply(simple_summaries, function(m) round(coef(m)[2, 4], 4))
) %>%
  kable(caption = "Simple Linear Regressions: $R^2$ and p-values", escape = FALSE, booktabs = TRUE) %>%
  kable_styling(font_size = 8, latex_options = "hold_position")
```

The table summarizes the strength of individual linear relationships
between each predictor (X1–X6) and the mortgage yield using simple
linear regression.

- **X1** has the **strongest linear association** with mortgage yield,
explaining approximately **65.4% of its variance** and is highly
significant (p \< 0.001)

- **X2** and **X3** also show strong and significant associations ($R^2$ =
0.546 and 0.517, respectively).

- **X5** and **X6** show moderate yet significant associations ($R^2$ =
0.419 and 0.346).

- **X4 (Savings per Capita)** does **not** show a significant
relationship with mortgage yield ($R^2$ = 0.049, p = 0.3763), suggesting it
may not be a strong individual predictor.

This preliminary analysis indicates that variables X1, X2, and X3 are
the most promising candidates for predicting mortgage yield in a
multivariate model.

## Null Model vs Full Model Comparison

```{r}
# Null Model vs Full Model Comparison
null_model <- lm(mortYld ~ 1, data = df)
full_model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)

anova(null_model, full_model)
```

The ANOVA comparison between the null model and the full model reveals
that the full model, which includes the predictors, significantly
improves the model fit. The null model (intercept-only) does not explain
much of the variation in mortgage yield.

The full model, provides a better explanation of the mortgage yield, as
shown by the significant F-statistic and the p-value. This indicates
that at least one of the predictors is significantly related to mortgage
yield, and the explanatory variables are useful for improving the model.

```{r}
# Fit linear regression model
model <- lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = df)

# Model summary
summary(model)

```

The model explains approximately **87%** of the variance in mortgage
yield, and after adjusting for the number of predictors, **80%** is
still explained. This is a strong fit. The overall model is
statistically significant, with a very low p-value. Once again, it means
that at least one predictor contributes significantly to explaining the
variation in mortYld. The intercept appears to be really significant to
fit the model.

However, most variables do not show statistically significant individual
<<<<<<< HEAD
contributions. Only X1 and X3 show strong significance (p %~~% 0.05). Other
variables, X2, X5 and X6, do not show strong individual effects. This
suggests that a reduced model may be more interpretable.
=======
contributions. Only X1 and X3 show strong significance (p $\approx$
0.05). Other variables, X2, X5 and X6, do not show strong individual
effects. This suggests that a reduced model may be more interpretable.
>>>>>>> 47138a17d1ee1c6bb3daa11dde3bd15baa14cc0d

## Make stepwise regression to select the best model

```{r}
# Stepwise regression (both directions)
step_model <- stepAIC(model, direction = "both")
summary(step_model)
```

(mettre en annexe ?)

The stepwise regression process identified X1, X3, and X4 as the most
significant predictors of mortality yield, leading to the final model.

It's interesting to see that X4 appears among the 3 most significant
predictoes although it shows the weakest correlation in the correlation
matrix. Multiple regression measures the effect of each variable while
holding all other constant. As X4 has very strong correlation with X3,
holding X3 can make the unique contribution of X4 clearer.

The final model explains approximately **83.4% of the variance** in
mortgage yield using only these three predictors.\
The AIC isn't increased by a lot when keeping the other variables, which
means that these are still statistically valid but not so useful. The
final is simpler but still explain the data just as well or better.

The residual standard error (0.091) is low, and the overall model is
highly significant, indicating a good fit.\
We end up with : mortYld = 4.223 + 0.02229.X1 - 0.001863.X3 +
0.0002249.X4\

Let's try a model with 2-way interactions.

```{r}
interaction_model <- lm(mortYld ~ X1 + X3 + X4 + X1:X3 + X1:X4 + X3:X4, data = df)
summary(interaction_model)
```

The interactions increase the complexity of the model for an improvement
that seems very small.

We decided not to include a 3-way interaction model in our analysis.
Given the small sample size (18 observations), adding high-order
interactions would significantly reduce degrees of freedom and increase
the risk of overfitting. Moreover, 3-way interactions are often
difficult to interpret meaningfully.

## Model Comparison

```{r, fig.width=10, fig.height=2}
# Create comparison table
model_metrics <- data.frame(
  Model = c("Full Model", "Stepwise Model", "2-Way Interaction Model"),
  R2 = c(
    summary(full_model)$r.squared,
    summary(step_model)$r.squared,
    summary(interaction_model)$r.squared
  ),
  Adj_R2 = c(
    summary(full_model)$adj.r.squared,
    summary(step_model)$adj.r.squared,
    summary(interaction_model)$adj.r.squared
  ),
  AIC = c(
    AIC(full_model),
    AIC(step_model),
    AIC(interaction_model)
  ),
  Residual_SE = c(
    summary(full_model)$sigma,
    summary(step_model)$sigma,
    summary(interaction_model)$sigma
  ),
  F_statistic = c(
    summary(full_model)$fstatistic[1],
    summary(step_model)$fstatistic[1],
    summary(interaction_model)$fstatistic[1]
  )
)

# Display table
kable(model_metrics, digits = 3, caption = "Comparison of Model Performance Metrics") %>%
  kable_styling(font_size = 8, full_width = FALSE, position = "center")
```

The **Stepwise Model** offers the best trade-off between simplicity and
performance: It has the **lowest AIC**, indicating the best model fit
among the three. Despite having a slightly lower **R²** than the full
model, it achieves the **highest Adjusted R²**.

It also has the **lowest residual standard error** and the **highest
F-statistic**, confirming overall model significance and parsimony.

```{r}
# Compare with ANOVA
anova(step_model, interaction_model)
```

An ANOVA was conducted to assess whether including 2-way interaction
terms significantly improved the model fit. The test yielded an
F-statistic of 0.18 and a p-value of 0.91, indicating that the
additional interaction terms did not meaningfully reduce the residual
variance.

As a result, we retained the simpler model with only main effects (X1,
X3, and X4), which offers comparable explanatory power and better
interpretability.

------------------------------------------------------------------------

# Model assumptions and Diagnostics

## Independence evaluation

```{r, fig.width=6, fig.height=3}
par(mfrow = c(1, 2))

# Residuals vs Fitted Values
plot(fitted(model), resid(model),
     main = "Residuals vs Fitted Values",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 19, col = "blue")

# Residuals vs Observation Order
plot(resid(model),
     main = "Residuals vs Observation Order",
     xlab = "Observation Index", ylab = "Residuals",
     pch = 19, col = "darkgreen")

# Reset layout
par(mfrow = c(1, 1))
```

The 1st graph shows that the residuals appear randomly scattered around
0. There's no clear pattern. This suggests the assumptions of linearity
and constant error variance (homoscedasticity) are reasonably met.

The 2nd graph can help us conclude that there is no consistent trend so
the independance is verified.

## Multicolinearity diagnostic

```{r, fig.width=8, fig.height=3}
# Compute Variance Inflation Factor (VIF)
vif_values <- vif(step_model)
kable(as.data.frame(vif_values), caption = "Variance Inflation Factors (VIF)") %>% 
  kable_styling(font_size = 8, latex_options = "hold_position")

```

All variables have a VIF value under 5 meaning that variables are not
too highly related and that no variable should be eliminated. This
confirms our choice of keeping X3 and X4 even if they showed a high
correlation coefficient.

## Homoscedasticity

Random scatter indicates good assumption of homeoscedasticity. If we can
distinguish a clear pattern, then we have potential heteroscedasticity
issue.

```{r, fig.width=4, fig.height=4}
par(pty = "s") 
plot(step_model$fitted.values, abs(resid(step_model)),
     main = "Scale-Location Plot",
     xlab = "Fitted values", ylab = "|Residuals|", pch = 19)
```

## Normality Check

If points lie on 45 degrees line, it means the residuals are normally
distributed. If we can see a curved pattern, then the normality
assumption is violated

```{r, fig.width=4, fig.height=4}
# Q-Q Plot with equal axis limits
par(pty = "s") 
qqnorm(resid(step_model), 
       main = "Q-Q Plot of Residuals", 
       xlim = c(-2, 2), ylim = c(-0.2, 0.2))  # Fix both axes to same limits
qqline(resid(step_model), col = "red")
```

------------------------------------------------------------------------

# Final estimated Model

```{r}

```

------------------------------------------------------------------------

# Conclusions

```         
•   The analysis showed that [mention significant predictors] have a strong relationship with mortgage yield.
•   The assumptions of linear regression were [state if met or violated].
•   The model provides [good/poor] predictive accuracy based on [R² and residual analysis].
•   Future improvements could involve [mention possible improvements like transformations, additional predictors, etc.].
```
